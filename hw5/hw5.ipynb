{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Biostat 257 Homework 5\n",
    "\n",
    "**Due June 5 @ 11:59PM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "versioninfo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we continue with the linear mixed effects model (LMM)\n",
    "$$\n",
    "    \\mathbf{Y}_i = \\mathbf{X}_i \\boldsymbol{\\beta} + \\mathbf{Z}_i \\boldsymbol{\\gamma} + \\boldsymbol{\\epsilon}_i, \\quad i=1,\\ldots,n,\n",
    "$$\n",
    "where   \n",
    "- $\\mathbf{Y}_i \\in \\mathbb{R}^{n_i}$ is the response vector of $i$-th individual,  \n",
    "- $\\mathbf{X}_i \\in \\mathbb{R}^{n_i \\times p}$ is the fixed effects predictor matrix of $i$-th individual,  \n",
    "- $\\mathbf{Z}_i \\in \\mathbb{R}^{n_i \\times q}$ is the random effects predictor matrix of $i$-th individual,  \n",
    "- $\\boldsymbol{\\epsilon}_i \\in \\mathbb{R}^{n_i}$ are multivariate normal $N(\\mathbf{0}_{n_i},\\sigma^2 \\mathbf{I}_{n_i})$,  \n",
    "- $\\boldsymbol{\\beta} \\in \\mathbb{R}^p$ are fixed effects, and  \n",
    "- $\\boldsymbol{\\gamma} \\in \\mathbb{R}^q$ are random effects assumed to be $N(\\mathbf{0}_q, \\boldsymbol{\\Sigma}_{q \\times q}$) independent of $\\boldsymbol{\\epsilon}_i$.\n",
    "\n",
    "The log-likelihood of the $i$-th datum $(\\mathbf{y}_i, \\mathbf{X}_i, \\mathbf{Z}_i)$ is \n",
    "$$\n",
    "    \\ell_i(\\boldsymbol{\\beta}, \\mathbf{L}, \\sigma_0^2) = - \\frac{n_i}{2} \\log (2\\pi) - \\frac{1}{2} \\log \\det \\boldsymbol{\\Omega}_i - \\frac{1}{2} (\\mathbf{y} - \\mathbf{X}_i \\boldsymbol{\\beta})^T \\boldsymbol{\\Omega}_i^{-1} (\\mathbf{y} - \\mathbf{X}_i \\boldsymbol{\\beta}),\n",
    "$$\n",
    "where\n",
    "$$\n",
    "    \\boldsymbol{\\Omega}_i = \\sigma^2 \\mathbf{I}_{n_i} + \\mathbf{Z}_i \\boldsymbol{\\Sigma} \\mathbf{Z}_i^T.\n",
    "$$\n",
    "Given $m$ independent data points $(\\mathbf{y}_i, \\mathbf{X}_i, \\mathbf{Z}_i)$, $i=1,\\ldots,m$, we seek the maximum likelihood estimate (MLE) by maximizing the log-likelihood\n",
    "$$\n",
    "\\ell(\\boldsymbol{\\beta}, \\boldsymbol{\\Sigma}, \\sigma_0^2) = \\sum_{i=1}^m \\ell_i(\\boldsymbol{\\beta}, \\boldsymbol{\\Sigma}, \\sigma_0^2).\n",
    "$$\n",
    "\n",
    "In HW4, we used the nonlinear programming (NLP) approach (Newton type algorithms) for optimization. In this assignment, we derive and implement an expectation-maximization (EM) algorithm for the same problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load necessary packages; make sure install them first\n",
    "using BenchmarkTools, Distributions, LinearAlgebra, Random, Revise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1. (10 pts) Refresher on normal-normal model\n",
    "\n",
    "Assume the conditional distribution\n",
    "$$\n",
    "\\mathbf{y} \\mid \\boldsymbol{\\gamma} \\sim N(\\mathbf{X} \\boldsymbol{\\beta} + \\mathbf{Z} \\boldsymbol{\\gamma}, \\sigma^2 \\mathbf{I}_n)\n",
    "$$\n",
    "and the prior distribution\n",
    "$$\n",
    "\\boldsymbol{\\gamma} \\sim N(\\mathbf{0}_q, \\boldsymbol{\\Sigma}).\n",
    "$$\n",
    "By the Bayes theorem, the posterior distribution is\n",
    "\\begin{eqnarray*}\n",
    "f(\\boldsymbol{\\gamma} \\mid \\mathbf{y}) &=& \\frac{f(\\mathbf{y} \\mid \\boldsymbol{\\gamma}) \\times f(\\boldsymbol{\\gamma})}{f(\\mathbf{y})}, \\end{eqnarray*}\n",
    "where $f$ denotes corresponding density. \n",
    "\n",
    "Show that the posterior distribution of random effects $\\boldsymbol{\\gamma}$ is a multivariate normal with mean\n",
    "\\begin{eqnarray*}\n",
    "\\mathbb{E} (\\boldsymbol{\\gamma} \\mid \\mathbf{y}) &=& \\sigma^{-2} (\\sigma^{-2} \\mathbf{Z}^T \\mathbf{Z} + \\boldsymbol{\\Sigma}^{-1})^{-1 } \\mathbf{Z}^T (\\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta}) \\\\\n",
    "&=& \\boldsymbol{\\Sigma} \\mathbf{Z}^T (\\mathbf{Z} \\boldsymbol{\\Sigma} \\mathbf{Z}^T + \\sigma^2 \\mathbf{I})^{-1} (\\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta})\n",
    "\\end{eqnarray*}\n",
    "and covariance\n",
    "\\begin{eqnarray*}\n",
    "\\text{Var} (\\boldsymbol{\\gamma} \\mid \\mathbf{y}) &=& (\\sigma^{-2} \\mathbf{Z}^T \\mathbf{Z} + \\boldsymbol{\\Sigma}^{-1})^{-1} \\\\\n",
    "&=& \\boldsymbol{\\Sigma} - \\boldsymbol{\\Sigma} \\mathbf{Z}^T (\\mathbf{Z} \\boldsymbol{\\Sigma} \\mathbf{Z}^T + \\sigma^2 \\mathbf{I})^{-1} \\mathbf{Z} \\boldsymbol{\\Sigma}.\n",
    "\\end{eqnarray*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: since we intend to identify the distribution (identify the kernel) and its means and variance, we need not find the exact pdf. Thus I will be using proportional equations henceforth. Fristly, since $\\mathbf{y}$ is conditioned upon, the denomerator $f(\\mathbf{y})$ is a constant (integral over the conditional probability). Thus we can proportion it out (in addition, to make it easier to read, I will be using \"exp{}\" instead of the standard $e^{f(x)}$ for exponentials): \n",
    "\n",
    "$$\n",
    "f(\\boldsymbol{\\gamma} \\mid \\mathbf{y}) = \\frac{f(\\mathbf{y} \\mid \\boldsymbol{\\gamma}) \\times f(\\boldsymbol{\\gamma})}{f(\\mathbf{y})} \\\\\n",
    "\\propto f(\\boldsymbol{\\gamma} \\mid \\mathbf{y}) = f(\\mathbf{y} \\mid \\boldsymbol{\\gamma}) \\times f(\\boldsymbol{\\gamma}) \\\\\n",
    "\\propto exp\\{-\\frac{1}{2\\sigma^2} (\\mathbf{y}-\\mathbf{X}\\boldsymbol{\\beta}-\\mathbf{Z}\\boldsymbol{\\gamma})^T(\\mathbf{y}-\\mathbf{X}\\boldsymbol{\\beta}-\\mathbf{Z}\\boldsymbol{\\gamma})\\} exp\\{-\\frac{1}{2}\\boldsymbol{\\gamma}^T\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\gamma}\\} \\\\\n",
    "$$\n",
    "For convenience, let $\\mathbf{r} = \\mathbf{y}-\\mathbf{X}\\boldsymbol{\\beta}$.\n",
    "$$\n",
    "\\propto exp\\{-\\frac{1}{2}[\\frac{1}{\\sigma^2}(\\mathbf{r}-\\mathbf{Z}\\boldsymbol{\\gamma})^T(\\mathbf{r}-\\mathbf{Z}\\boldsymbol{\\gamma}) + \\boldsymbol{\\gamma}^T\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\gamma}]\\} \\\\\n",
    "\\propto exp\\{-\\frac{1}{2}[\\frac{1}{\\sigma^2}\\mathbf{r}^T\\mathbf{r} + \\frac{1}{\\sigma^2}\\boldsymbol{\\gamma}^T\\mathbf{Z}^T\\mathbf{Z}\\boldsymbol{\\gamma} - \\frac{2}{\\sigma^2}\\mathbf{r}^T\\mathbf{Z}\\boldsymbol{\\gamma} + \\boldsymbol{\\gamma}^T\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\gamma}]\\}\\\\ \n",
    "\\propto exp\\{-\\frac{1}{2}[\\frac{1}{\\sigma^2}\\mathbf{r}^T\\mathbf{r} + \\boldsymbol{\\gamma}^T(\\frac{1}{\\sigma^2}\\mathbf{Z}^T\\mathbf{Z} + \\boldsymbol{\\Sigma}^{-1})\\boldsymbol{\\gamma} - \\frac{2}{\\sigma^2}\\mathbf{r}^T\\mathbf{Z}\\boldsymbol{\\gamma}]\\}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, for convenience, let $\\mathbf{M}=(\\sigma^{-2}\\mathbf{Z}^T\\mathbf{Z} + \\boldsymbol{\\Sigma}^{-1})$, and $\\mathbf{b} = \\sigma^{-2}\\mathbf{r}^T\\mathbf{Z}$, and proportion out the first term in the last equation, since it does not contain $\\boldsymbol{\\gamma}$, and complete the square,\n",
    "$$\n",
    "f(\\boldsymbol{\\gamma} \\mid \\mathbf{y}) = \\frac{f(\\mathbf{y} \\mid \\boldsymbol{\\gamma}) \\times f(\\boldsymbol{\\gamma})}{f(\\mathbf{y})} \\\\\n",
    "\\propto exp\\{-\\frac{1}{2}[\\boldsymbol{\\gamma}^T\\mathbf{M}\\boldsymbol{\\gamma} - 2\\mathbf{b}\\boldsymbol{\\gamma}]\\} \\\\\n",
    "\\propto exp\\{-\\frac{1}{2}[\\boldsymbol{\\gamma}^T\\mathbf{M}\\boldsymbol{\\gamma} - 2\\mathbf{M}\\mathbf{M}^{-1}\\mathbf{b}\\boldsymbol{\\gamma}]\\} \\\\\n",
    "\\propto exp\\{-\\frac{1}{2}[\\boldsymbol{\\gamma}^T\\mathbf{M}\\boldsymbol{\\gamma} - 2\\mathbf{M}\\mathbf{M}^{-1}\\mathbf{b}\\boldsymbol{\\gamma} + \\mathbf{b}^T\\mathbf{M}^{-1}\\mathbf{b} - \\mathbf{b}^T\\mathbf{M}^{-1}\\mathbf{b}]\\} \\\\\n",
    "\\propto exp\\{-\\frac{1}{2}[(\\boldsymbol{\\gamma} - \\mathbf{M}^{-1}\\mathbf{b}^T)^T\\mathbf{M}(\\boldsymbol{\\gamma} - \\mathbf{M}^{-1}\\mathbf{b}^T) - \\mathbf{b}^T\\mathbf{M}^{-1}\\mathbf{b}]\\}\n",
    "$$\n",
    "And again, disregard the last term as it does not contain $\\boldsymbol{\\gamma}$.\n",
    "$$\n",
    "\\propto exp\\{-\\frac{1}{2}(\\boldsymbol{\\gamma} - \\mathbf{M}^{-1}\\mathbf{b}^T)^T\\mathbf{M}(\\boldsymbol{\\gamma} - \\mathbf{M}^{-1}\\mathbf{b}^T)\\} \\sim N(\\mathbf{M}^{-1}\\mathbf{b}^T, \\mathbf{M}^{-1})\n",
    "$$\n",
    "Therefore,\n",
    "$$\n",
    "\\implies \\mathbb{E} (\\boldsymbol{\\gamma} \\mid \\mathbf{y}) = \\mathbf{M}^{-1}\\mathbf{b}^T \\\\\n",
    "= (\\sigma^{-2}\\mathbf{Z}^T\\mathbf{Z} + \\boldsymbol{\\Sigma}^{-1})^{-1}\\sigma^{-2}\\mathbf{Z}^T\\mathbf{r} \\\\ \n",
    "= \\sigma^{-2}(\\sigma^{-2}\\mathbf{Z}^T\\mathbf{Z} + \\boldsymbol{\\Sigma}^{-1})^{-1}\\mathbf{Z}^T(\\mathbf{y}-\\mathbf{X}\\boldsymbol{\\beta}) \\\\\n",
    "= \\boldsymbol{\\Sigma} \\mathbf{Z}^T (\\mathbf{Z} \\boldsymbol{\\Sigma} \\mathbf{Z}^T + \\sigma^2 \\mathbf{I})^{-1} (\\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta}).\n",
    "$$\n",
    "And,\n",
    "$$\n",
    "\\implies \\text{Var} (\\boldsymbol{\\gamma} \\mid \\mathbf{y}) = \\mathbf{M}^{-1} \\\\\n",
    "= (\\sigma^{-2}\\mathbf{Z}^T\\mathbf{Z} + \\boldsymbol{\\Sigma}^{-1})^{-1} \\\\\n",
    "= \\boldsymbol{\\Sigma} - \\boldsymbol{\\Sigma} \\mathbf{Z}^T (\\mathbf{Z} \\boldsymbol{\\Sigma} \\mathbf{Z}^T + \\sigma^2 \\mathbf{I})^{-1} \\mathbf{Z} \\boldsymbol{\\Sigma}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2. (20 pts) Derive EM algorithm\n",
    "\n",
    "1. Write down the complete log-likelihood\n",
    "$$\n",
    "\\sum_{i=1}^m \\log f(\\mathbf{y}_i, \\boldsymbol{\\gamma}_i \\mid \\boldsymbol{\\beta}, \\boldsymbol{\\Sigma}, \\sigma^2)\n",
    "$$\n",
    "\n",
    "From earlier Baye's Theroem,\n",
    "$$\n",
    "= \\sum_{i=1}^m \\log f(\\mathbf{y}_i \\mid \\boldsymbol{\\gamma}_i, \\boldsymbol{\\beta}, \\boldsymbol{\\Sigma}, \\sigma^2) f(\\boldsymbol{\\gamma}_i \\mid \\boldsymbol{\\beta}, \\boldsymbol{\\Sigma}, \\sigma^2) \\\\\n",
    "= \\sum_{i=1}^m \\log [ (2\\pi\\sigma^2)^{-\\frac{n_i}{2}} (2\\pi)^{-\\frac{2}{q}}det(\\Sigma)^{-\\frac{1}{2}} e^{-\\frac{1}{2\\sigma^2} (\\mathbf{y}_i-\\mathbf{X}_i\\boldsymbol{\\beta}-\\mathbf{Z}_i\\boldsymbol{\\gamma})^T_i(\\mathbf{y}_i-\\mathbf{X}_i\\boldsymbol{\\beta}-\\mathbf{Z}_i\\boldsymbol{\\gamma}_i)} e^{-\\frac{1}{2}\\boldsymbol{\\gamma}^T_i\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\gamma}_i}] \\\\\n",
    "= \\sum_{i=1}^m [ -\\frac{n_i}{2}\\log(2\\pi\\sigma^2) - \\frac{q}{2}\\log(2\\pi) - \\frac{1}{2}\\log det(\\Sigma) -\\frac{1}{2\\sigma^2} (\\mathbf{y}_i-\\mathbf{X}_i\\boldsymbol{\\beta}-\\mathbf{Z}_i\\boldsymbol{\\gamma}_i)^T(\\mathbf{y}_i-\\mathbf{X}_i\\boldsymbol{\\beta}-\\mathbf{Z}_i\\boldsymbol{\\gamma}_i) - \\frac{1}{2}\\boldsymbol{\\gamma}^T_i\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\gamma}_i]\n",
    "$$\n",
    "\n",
    "Now to prepare for the second part, I will isolate and group $\\boldsymbol{\\gamma}_i$, apart from the terms that do not involve $\\boldsymbol{\\gamma}_i$. This way when we make it easier when taking the expectation in the E step,\n",
    "$$\n",
    "= - \\frac{m}{2}\\log det(\\boldsymbol{\\Sigma}) + \\sum_{i=1}^m \\{ c_i -\\frac{n_i}{2}\\log(\\sigma^2) - \\frac{1}{2}[ \\sigma^{-2}\\mathbf{r}^T_i\\mathbf{r}_i + \\boldsymbol{\\gamma}^T_i(\\frac{1}{\\sigma^2}\\mathbf{Z}^T_i\\mathbf{Z}_i + \\boldsymbol{\\Sigma}^{-1})\\boldsymbol{\\gamma}_i - \\frac{2}{\\sigma^2}\\mathbf{r}^T_i\\mathbf{Z}_i\\boldsymbol{\\gamma}_i] \\} \\\\\n",
    "= - \\frac{m}{2}\\log det(\\Sigma) + \\sum_{i=1}^m c_i -\\sum_{i=1}^m\\frac{n_i}{2}\\log(\\sigma^2) - \\frac{1}{2\\sigma^{2}}\\sum_{i=1}^m  \\mathbf{r}^T_i\\mathbf{r}_i - \\frac{1}{2}\\sum_{i=1}^m \\boldsymbol{\\gamma}^T_i(\\frac{1}{\\sigma^2}\\mathbf{Z}^T_i\\mathbf{Z}_i + \\boldsymbol{\\Sigma}^{-1})\\boldsymbol{\\gamma}_i + \\frac{1}{\\sigma^2} \\sum_{i=1}^m \\mathbf{r}^T_i\\mathbf{Z}_i\\boldsymbol{\\gamma}_i\n",
    "$$\n",
    "where the convenience notations are the same, $\\mathbf{r}_i= \\mathbf{y}_i-\\mathbf{X}_i\\boldsymbol{\\beta}$, and $\\mathbf{M}_i=(\\sigma^{-2}\\mathbf{Z}^T_i\\mathbf{Z}_i + \\boldsymbol{\\Sigma}^{-1})$ as defined in Q1.    \n",
    "\n",
    "2. Derive the $Q$ function (E-step).\n",
    "\n",
    "$$\n",
    "Q(\\boldsymbol{\\beta}, \\boldsymbol{\\Sigma}, \\sigma^2 \\mid \\boldsymbol{\\beta}^{(t)}, \\boldsymbol{\\Sigma}^{(t)}, \\sigma^{2(t)})\n",
    "$$\n",
    "Answer: Take the expectation of the expression from part 1 above, bringing terms not involving $\\boldsymbol{\\gamma}_i$ outside of the expectation. Thus,    \n",
    "\n",
    "$$\n",
    "= \\mathbb{E}_{\\boldsymbol{\\gamma}_i \\mid \\mathbf{y}_i} [\\sum_{i=1}^m \\log f(\\mathbf{y}_i, \\boldsymbol{\\gamma}_i \\mid \\boldsymbol{\\beta}^{(t)}, \\boldsymbol{\\Sigma}^{(t)}, \\sigma^2) \\mid \\mathbf{Y} = \\mathbf{y}, \\boldsymbol{\\beta}^{(t)}, \\boldsymbol{\\Sigma}^{(t)}, \\sigma^{2(t)}] \\\\\n",
    "= \\sum_{i=1}^m \\mathbb{E}_{\\boldsymbol{\\gamma}_i \\mid \\mathbf{y}_i} [ - \\frac{1}{2}\\log det(\\boldsymbol{\\Sigma}) + c_i - \\frac{n_i}{2}\\log(\\sigma^2) - \\frac{1}{2\\sigma^{2}} \\mathbf{r}^T_i\\mathbf{r}_i - \\frac{1}{2} \\boldsymbol{\\gamma}^T_i(\\frac{1}{\\sigma^2}\\mathbf{Z}^T_i\\mathbf{Z}_i + \\boldsymbol{\\Sigma}^{-1})\\boldsymbol{\\gamma}_i + \\frac{1}{\\sigma^2} \\mathbf{r}^T_i\\mathbf{Z}_i\\boldsymbol{\\gamma}_i \\mid \\mathbf{Y} = \\mathbf{y}, \\boldsymbol{\\beta}^{(t)}, \\boldsymbol{\\Sigma}^{(t)}, \\sigma^{2(t)}] \\\\\n",
    "= \\sum_{i=1}^m [- \\frac{1}{2}\\log det(\\Sigma) + c_i - \\frac{n_i}{2}\\log(\\sigma^2) - \\frac{1}{2\\sigma^{2}} \\mathbf{r}^T_i\\mathbf{r}_i ] + \\sum_{i=1}^m \\mathbb{E}_{\\boldsymbol{\\gamma}_i \\mid \\mathbf{y}_i} [ - \\frac{1}{2} \\boldsymbol{\\gamma}^T_i(\\frac{1}{\\sigma^2}\\mathbf{Z}^T_i\\mathbf{Z}_i + \\boldsymbol{\\Sigma}^{-1})\\boldsymbol{\\gamma}_i + \\frac{1}{\\sigma^2} \\mathbf{r}^T_i\\mathbf{Z}_i\\boldsymbol{\\gamma}_i \\mid \\mathbf{Y} = \\mathbf{y}, \\boldsymbol{\\beta}^{(t)}, \\boldsymbol{\\Sigma}^{(t)}, \\sigma^{2(t)}]\n",
    "$$\n",
    "Use the property (trivial to prove) $\\mathbb{E}[\\mathbf{xAx}]=tr(\\mathbf{A}\\boldsymbol{\\Sigma}) + \\boldsymbol{\\mu}^T\\mathbf{A}\\boldsymbol{\\mu}$, we have,     \n",
    "\n",
    "$$\n",
    "= \\sum_{i=1}^m [- \\frac{1}{2}\\log det(\\boldsymbol{\\Sigma}) + c_i - \\frac{n_i}{2}\\log(\\sigma^2) - \\frac{1}{2\\sigma^{2}} \\mathbf{r}^T_i\\mathbf{r}_i ] -\\frac{1}{2}\\sum_{i=1}^m tr[(\\frac{1}{\\sigma^2}\\mathbf{Z}^T_i\\mathbf{Z}_i + \\boldsymbol{\\Sigma}^{-1})\\text{Var} (\\boldsymbol{\\gamma}_i \\mid \\mathbf{y})] - \\frac{1}{2}\\sum_{i=1}^m\\boldsymbol{\\mu}^T_{\\boldsymbol{\\gamma} \\mid \\mathbf{y}}(\\frac{1}{\\sigma^2}\\mathbf{Z}^T_i\\mathbf{Z}_i + \\boldsymbol{\\Sigma}^{-1})\\boldsymbol{\\mu}_{\\boldsymbol{\\gamma} \\mid \\mathbf{y}} \\\\\n",
    "+ \\sigma^{-2}\\sum_{i=1}^m \\mathbf{r}^T_i\\mathbf{Z}_i\\boldsymbol{\\mu}_{\\boldsymbol{\\gamma} \\mid \\mathbf{y}}\n",
    "$$\n",
    "Take out the terms that do not contain $\\boldsymbol{\\beta}$, $\\boldsymbol{\\Sigma}$, or $\\sigma^2$ that we try to update, and obsorb them into $C_i$.\n",
    "$$\n",
    "=- \\frac{m}{2}\\log det(\\boldsymbol{\\Sigma}) + \\sum_{i=1}^m c_i - \\sum_{i=1}^m\\frac{n_i}{2}\\log(\\sigma^2) - \\frac{1}{2\\sigma^{2}} \\sum_{i=1}^m \\mathbf{r}^T_i\\mathbf{r}_i \\\\\n",
    "- \\frac{1}{2\\sigma^2}\\sum_{i=1}^m tr[\\mathbf{Z}^T_i\\mathbf{Z}_i \\text{Var} (\\boldsymbol{\\gamma}_i \\mid \\mathbf{y})]\n",
    "- \\frac{1}{2}\\sum_{i=1}^m tr[\\boldsymbol{\\Sigma}^{-1}\\text{Var}(\\boldsymbol{\\gamma}_i \\mid \\mathbf{y})] \\\\\n",
    "- \\frac{1}{2\\sigma^2}\\sum_{i=1}^m\\boldsymbol{\\mu}^T_{\\boldsymbol{\\gamma} \\mid \\mathbf{y}}\\mathbf{Z}^T_i\\mathbf{Z}_i\\boldsymbol{\\mu}_{\\boldsymbol{\\gamma} \\mid \\mathbf{y}} \n",
    "- \\frac{1}{2}\\sum_{i=1}^m\\boldsymbol{\\mu}^T_{\\boldsymbol{\\gamma} \\mid \\mathbf{y}}\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\mu}_{\\boldsymbol{\\gamma} \\mid \\mathbf{y}} \\\\\n",
    "+ \\sigma^{-2}\\sum_{i=1}^m \\mathbf{y}^T_i\\mathbf{Z}_i\\boldsymbol{\\mu}^T_{\\boldsymbol{\\gamma} \\mid \\mathbf{y}} - \\sigma^{-2}\\sum_{i=1}^m \\boldsymbol{\\beta}^T\\mathbf{X}^T_i\\mathbf{Z}_i\\boldsymbol{\\mu}_{\\boldsymbol{\\gamma} \\mid \\mathbf{y}} \n",
    "$$\n",
    "\n",
    "3. Derive the EM (or ECM) update of $\\boldsymbol{\\beta}$, $\\boldsymbol{\\Sigma}$, and $\\sigma^2$ (M-step). \n",
    "Solution: first find gradient for $\\boldsymbol{\\Sigma}$\n",
    "$$\n",
    "\\mathbf{D}_{\\boldsymbol{\\Sigma}} \\ell_{\\boldsymbol{\\gamma} \\mid \\mathbf{y}} = \\\\\n",
    "\\mathbf{D}_{\\boldsymbol{\\Sigma}} \\{ - \\frac{m}{2}\\log det(\\boldsymbol{\\Sigma}) - \\frac{1}{2}\\sum_{i=1}^m tr[\\boldsymbol{\\Sigma}^{-1}\\text{Var}(\\boldsymbol{\\gamma}_i \\mid \\mathbf{y})] - \\frac{1}{2}\\sum_{i=1}^m\\boldsymbol{\\mu}^T_{\\boldsymbol{\\gamma}_i \\mid \\mathbf{y}}\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\mu}_{\\boldsymbol{\\gamma}_i \\mid \\mathbf{y}} \\} \\\\\n",
    "= \\mathbf{D}_{\\boldsymbol{\\Sigma}} \\{ - \\frac{m}{2}\\log det(\\boldsymbol{\\Sigma}) - \\frac{1}{2}\\sum_{i=1}^m tr(\\mathbf{V}_i\\boldsymbol{\\Sigma}^{-1}) - \\frac{1}{2}\\sum_{i=1}^m tr(\\boldsymbol{\\mu}_{\\boldsymbol{\\gamma}_i \\mid \\mathbf{y}}\\boldsymbol{\\mu}^T_{\\boldsymbol{\\gamma}_i \\mid \\mathbf{y}}\\boldsymbol{\\Sigma}^{-1}) \\} \\\\\n",
    "= - \\frac{m}{2det(\\boldsymbol{\\Sigma})} det(\\boldsymbol{\\Sigma})\\boldsymbol{\\Sigma}^{-1} + \\frac{1}{2}\\sum_{i=1}^m \\boldsymbol{\\Sigma}^{-1}\\mathbf{V}_i\\boldsymbol{\\Sigma}^{-1} + \\frac{1}{2}\\sum_{i=1}^m \\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\mu}_{\\boldsymbol{\\gamma}_i \\mid \\mathbf{y}}\\boldsymbol{\\mu}^T_{\\boldsymbol{\\gamma}_i \\mid \\mathbf{y}}\\boldsymbol{\\Sigma}^{-1} \\\\\n",
    "= - \\frac{m}{2}\\boldsymbol{\\Sigma}^{-1} + \\frac{1}{2} \\boldsymbol{\\Sigma}^{-1}(\\sum_{i=1}^m\\mathbf{V}_i)\\boldsymbol{\\Sigma}^{-1} + \\frac{1}{2}\\boldsymbol{\\Sigma}^{-1}(\\sum_{i=1}^m \\boldsymbol{\\mu}_{\\boldsymbol{\\gamma}_i \\mid \\mathbf{y}}\\boldsymbol{\\mu}^T_{\\boldsymbol{\\gamma}_i \\mid \\mathbf{y}})\\boldsymbol{\\Sigma}^{-1} \\\\\n",
    "= - \\frac{m}{2}\\boldsymbol{\\Sigma}^{-1} + \\frac{1}{2} \\boldsymbol{\\Sigma}^{-1}(\\sum_{i=1}^m\\mathbf{V}_i + \\boldsymbol{\\mu}_{\\boldsymbol{\\gamma}_i \\mid \\mathbf{y}}\\boldsymbol{\\mu}^T_{\\boldsymbol{\\gamma}_i \\mid \\mathbf{y}})\\boldsymbol{\\Sigma}^{-1} \\\\\n",
    "\\text{set to} = 0 \\\\\n",
    "\\implies \\frac{1}{2} \\boldsymbol{\\Sigma}^{-1}(\\sum_{i=1}^m\\mathbf{V}_i + \\boldsymbol{\\mu}_{\\boldsymbol{\\gamma}_i \\mid \\mathbf{y}}\\boldsymbol{\\mu}^T_{\\boldsymbol{\\gamma}_i \\mid \\mathbf{y}})\\boldsymbol{\\Sigma}^{-1} = \\frac{m}{2}\\boldsymbol{\\Sigma}^{-1} \\\\\n",
    "\\implies  \\boldsymbol{\\Sigma}^{-1}\\sum_{i=1}^m (\\mathbf{V}_i + \\boldsymbol{\\mu}_{\\boldsymbol{\\gamma}_i \\mid \\mathbf{y}}\\boldsymbol{\\mu}^T_{\\boldsymbol{\\gamma}_i \\mid \\mathbf{y}}) = m\\mathbf{I} \\\\\n",
    "\\implies \\boldsymbol{\\Sigma}^{(t+1)} = \\frac{1}{m}\\sum_{i=1}^m(\\mathbf{V}_i + \\boldsymbol{\\mu}_{\\boldsymbol{\\gamma}_i \\mid \\mathbf{y}}\\boldsymbol{\\mu}^T_{\\boldsymbol{\\gamma}_i \\mid \\mathbf{y}})\n",
    "$$\n",
    "where $\\mathbf{V}_i = \\text{Var}(\\boldsymbol{\\gamma}_i \\mid \\mathbf{y})$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{D}_{\\boldsymbol{\\beta}} \\ell_{\\boldsymbol{\\gamma} \\mid \\mathbf{y}} = \\mathbf{D}_{\\boldsymbol{\\beta}} \\{ - \\frac{1}{2\\sigma^{2}} \\sum_{i=1}^m (\\boldsymbol{\\beta}^T\\mathbf{X}^T_i\\mathbf{X}_i\\boldsymbol{\\beta} - 2\\boldsymbol{\\beta}^T\\mathbf{X}^T_i\\mathbf{y}_i) + \\sigma^{-2}\\sum_{i=1}^m \\boldsymbol{\\beta}^T\\mathbf{X}^T_i\\mathbf{Z}_i\\boldsymbol{\\mu}_{\\boldsymbol{\\gamma}_i \\mid \\mathbf{y}} \\} \\\\\n",
    "= - \\frac{1}{2\\sigma^{2}} \\sum_{i=1}^m (2\\mathbf{X}^T_i\\mathbf{X}_i\\boldsymbol{\\beta} - 2\\mathbf{X}^T_i\\mathbf{y}_i) + \\sigma^{-2}\\sum_{i=1}^m \\mathbf{X}^T_i\\mathbf{Z}_i\\boldsymbol{\\mu}_{\\boldsymbol{\\gamma}_i \\mid \\mathbf{y}} \\\\\n",
    "= - \\frac{1}{\\sigma^{2}} \\sum_{i=1}^m \\mathbf{X}^T_i\\mathbf{X}_i\\boldsymbol{\\beta} + \\frac{1}{\\sigma^{2}}\\sum_{i=1}^m (\\mathbf{X}^T_i\\mathbf{y}_i + \\mathbf{X}^T_i\\mathbf{Z}_i\\boldsymbol{\\mu}_{\\boldsymbol{\\gamma}_i \\mid \\mathbf{y}})  \\\\\n",
    "\\text{set to} = 0 \\\\\n",
    "\\implies \\sum_{i=1}^m (\\mathbf{X}^T_i\\mathbf{X}_i)\\boldsymbol{\\beta} = \\sum_{i=1}^m (\\mathbf{X}^T_i\\mathbf{y}_i + \\mathbf{X}^T_i\\mathbf{Z}_i\\boldsymbol{\\mu}_{\\boldsymbol{\\gamma}_i \\mid \\mathbf{y}}) \\\\\n",
    "\\boldsymbol{\\beta}^{(t+1)} = (\\sum_{i=1}^m \\mathbf{X}^T_i\\mathbf{X}_i)^{-1}\\sum_{i=1}^m (\\mathbf{X}^T_i\\mathbf{y}_i + \\mathbf{X}^T_i\\mathbf{Z}_i\\boldsymbol{\\mu}_{\\boldsymbol{\\gamma}_i \\mid \\mathbf{y}})\n",
    "$$\n",
    "\n",
    "Lastly, \n",
    "\n",
    "$$\n",
    "\\mathbf{D}_{\\sigma^2} \\ell_{\\boldsymbol{\\gamma} \\mid \\mathbf{y}} = \\mathbf{D}_{\\sigma^2} \\{ - \\frac{1}{2\\sigma^{2}} \\sum_{i=1}^m \\mathbf{r}^T_i\\mathbf{r}_i - \\frac{1}{2\\sigma^2}\\sum_{i=1}^m tr[\\mathbf{Z}^T_i\\mathbf{Z}_i \\text{Var} (\\boldsymbol{\\gamma}_i \\mid \\mathbf{y})]\n",
    "- \\frac{1}{2\\sigma^2}\\sum_{i=1}^m\\boldsymbol{\\mu}^T_{\\boldsymbol{\\gamma} \\mid \\mathbf{y}}\\mathbf{Z}^T_i\\mathbf{Z}_i\\boldsymbol{\\mu}_{\\boldsymbol{\\gamma} \\mid \\mathbf{y}} \n",
    "+ \\sigma^{-2}\\sum_{i=1}^m \\mathbf{y}^T_i\\mathbf{Z}_i\\boldsymbol{\\mu}^T_{\\boldsymbol{\\gamma} \\mid \\mathbf{y}} - \\sigma^{-2}\\sum_{i=1}^m \\boldsymbol{\\beta}^T\\mathbf{X}^T_i\\mathbf{Z}_i\\boldsymbol{\\mu}^T_{\\boldsymbol{\\gamma} \\mid \\mathbf{y}} \\\\\n",
    "- \\frac{\\log(\\sigma^2)}{2}\\sum_{i=1}^m n_i  \\} \\\\\n",
    "= \\frac{1}{2(\\sigma^2)^2} (- \\sum_{i=1}^m \\mathbf{r}^T_i\\mathbf{r}_i - \\sum_{i=1}^m tr[\\mathbf{Z}^T_i\\mathbf{Z}_i \\text{Var} (\\boldsymbol{\\gamma}_i \\mid \\mathbf{y})]\n",
    "- \\sum_{i=1}^m\\boldsymbol{\\mu}^T_{\\boldsymbol{\\gamma} \\mid \\mathbf{y}}\\mathbf{Z}^T_i\\mathbf{Z}_i\\boldsymbol{\\mu}_{\\boldsymbol{\\gamma} \\mid \\mathbf{y}} \n",
    "+ 2\\sum_{i=1}^m \\mathbf{y}^T_i\\mathbf{Z}_i\\boldsymbol{\\mu}^T_{\\boldsymbol{\\gamma} \\mid \\mathbf{y}} - 2\\sum_{i=1}^m \\boldsymbol{\\beta}^T\\mathbf{X}^T_i\\mathbf{Z}_i\\boldsymbol{\\mu}^T_{\\boldsymbol{\\gamma} \\mid \\mathbf{y}}) - \\frac{1}{2\\sigma^2}\\sum_{i=1}^m n_i \\\\\n",
    "\\text{set to} = 0 \\\\\n",
    "\\implies \\sigma^{2(t+1)} = \\frac{1}{\\sum_{i=1}^m n_i} (- \\sum_{i=1}^m \\mathbf{r}^T_i\\mathbf{r}_i - \\sum_{i=1}^m tr[\\mathbf{Z}^T_i\\mathbf{Z}_i \\text{Var} (\\boldsymbol{\\gamma}_i \\mid \\mathbf{y})]\n",
    "- \\sum_{i=1}^m\\boldsymbol{\\mu}^T_{\\boldsymbol{\\gamma} \\mid \\mathbf{y}}\\mathbf{Z}^T_i\\mathbf{Z}_i\\boldsymbol{\\mu}_{\\boldsymbol{\\gamma} \\mid \\mathbf{y}} \n",
    "+ 2\\sum_{i=1}^m \\mathbf{y}^T_i\\mathbf{Z}_i\\boldsymbol{\\mu}^T_{\\boldsymbol{\\gamma} \\mid \\mathbf{y}} - 2\\sum_{i=1}^m \\boldsymbol{\\beta}^T\\mathbf{X}^T_i\\mathbf{Z}_i\\boldsymbol{\\mu}^T_{\\boldsymbol{\\gamma} \\mid \\mathbf{y}})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3. (20 pts) Objective of a single datum\n",
    "\n",
    "We modify the code from HW4 to evaluate the objective, the conditional mean of $\\boldsymbol{\\gamma}$, and the conditional variance of $\\boldsymbol{\\gamma}$. Start-up code is provided below. You do _not_ have to use this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "logl!"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define a type that holds an LMM datum\n",
    "struct LmmObs{T <: AbstractFloat}\n",
    "    # data\n",
    "    y          :: Vector{T}\n",
    "    X          :: Matrix{T}\n",
    "    Z          :: Matrix{T}\n",
    "    # posterior mean and variance of random effects γ\n",
    "    μγ         :: Vector{T} # posterior mean of random effects\n",
    "    νγ         :: Matrix{T} # posterior variance of random effects\n",
    "    # TODO: add whatever intermediate arrays you may want to pre-allocate\n",
    "    yty        :: T\n",
    "    rtr        :: Vector{T}\n",
    "    xty        :: Vector{T}\n",
    "    zty        :: Vector{T}\n",
    "    ztr        :: Vector{T}\n",
    "    ltztr      :: Vector{T}\n",
    "    xtr        :: Vector{T}\n",
    "    storage_p  :: Vector{T}\n",
    "    storage_q  :: Vector{T}\n",
    "    storage_q2  :: Vector{T}\n",
    "    xtx        :: Matrix{T}\n",
    "    ztx        :: Matrix{T}\n",
    "    ztz        :: Matrix{T}\n",
    "    ltztzl     :: Matrix{T}\n",
    "    storage_qq  :: Matrix{T}\n",
    "    storage_qq2  :: Matrix{T}\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "    LmmObs(y::Vector, X::Matrix, Z::Matrix)\n",
    "\n",
    "Create an LMM datum of type `LmmObs`.\n",
    "\"\"\"\n",
    "function LmmObs(\n",
    "    y::Vector{T}, \n",
    "    X::Matrix{T}, \n",
    "    Z::Matrix{T}) where T <: AbstractFloat\n",
    "    n, p, q = size(X, 1), size(X, 2), size(Z, 2)\n",
    "    μγ         = Vector{T}(undef, q)\n",
    "    νγ         = Matrix{T}(undef, q, q)\n",
    "    yty        = abs2(norm(y))\n",
    "    rtr        = Vector{T}(undef, 1)\n",
    "    xty        = transpose(X) * y\n",
    "    zty        = transpose(Z) * y\n",
    "    ztr        = similar(zty)\n",
    "    ltztr      = similar(zty)\n",
    "    xtr        = Vector{T}(undef, p)\n",
    "    storage_p  = similar(xtr)\n",
    "    storage_q  = Vector{T}(undef, q)\n",
    "    storage_q2  = Vector{T}(undef, q)\n",
    "    xtx        = transpose(X) * X\n",
    "    ztx        = transpose(Z) * X\n",
    "    ztz        = transpose(Z) * Z\n",
    "    ltztzl     = similar(ztz)\n",
    "    storage_qq = similar(ztz)\n",
    "    storage_qq2 = similar(ztz)\n",
    "    LmmObs(y, X, Z, μγ, νγ, \n",
    "        yty, rtr, xty, zty, ztr, ltztr, xtr,\n",
    "        storage_p, storage_q, storage_q2, \n",
    "        xtx, ztx, ztz, ltztzl, storage_qq, storage_qq2)\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "    logl!(obs::LmmObs, β, Σ, L, σ², updater = false)\n",
    "\n",
    "Evaluate the log-likelihood of a single LMM datum at parameter values `β`, `Σ`, \n",
    "and `σ²`. The lower triangular Cholesky factor `L` of `Σ` must be supplied too.\n",
    "The fields `obs.μγ` and `obs.νγ` are overwritten by the posterior mean and \n",
    "posterior variance of random effects. If `updater==true`, fields `obs.ztr`, \n",
    "`obs.xtr`, and `obs.rtr` are updated according to input parameter values. \n",
    "Otherwise, it assumes these three fields are pre-computed. \n",
    "\"\"\"\n",
    "function logl!(\n",
    "        obs     :: LmmObs{T}, \n",
    "        β       :: Vector{T}, \n",
    "        Σ       :: Matrix{T},\n",
    "        L       :: Matrix{T},\n",
    "        σ²      :: T,\n",
    "        updater :: Bool = false\n",
    "        ) where T <: AbstractFloat\n",
    "    n, p, q = size(obs.X, 1), size(obs.X, 2), size(obs.Z, 2)\n",
    "    σ²inv   = inv(σ²)\n",
    "    ####################\n",
    "    # Evaluate objective\n",
    "    ####################\n",
    "    # form the q-by-q matrix: Lt Zt Z L\n",
    "    copy!(obs.ltztzl, obs.ztz)\n",
    "    BLAS.trmm!('L', 'L', 'T', 'N', T(1), L, obs.ltztzl) # O(q^3)\n",
    "    BLAS.trmm!('R', 'L', 'N', 'N', T(1), L, obs.ltztzl) # O(q^3)        \n",
    "    # form the q-by-q matrix: M = σ² I + Lt Zt Z L\n",
    "    copy!(obs.storage_qq, obs.ltztzl)\n",
    "    @inbounds for j in 1:q\n",
    "        obs.storage_qq[j, j] += σ²\n",
    "    end\n",
    "    LAPACK.potrf!('U', obs.storage_qq) # O(q^3)\n",
    "    # Zt * res\n",
    "    updater && BLAS.gemv!('N', T(-1), obs.ztx, β, T(1), copy!(obs.ztr, obs.zty)) # O(pq)\n",
    "    # Lt * (Zt * res)\n",
    "    BLAS.trmv!('L', 'T', 'N', L, copy!(obs.ltztr, obs.ztr))    # O(q^2)\n",
    "    # storage_q = (Mchol.U') \\ (Lt * (Zt * res))\n",
    "    BLAS.trsv!('U', 'T', 'N', obs.storage_qq, copy!(obs.storage_q, obs.ltztr)) # O(q^3)\n",
    "    # Xt * res = Xt * y - Xt * X * β\n",
    "    updater && BLAS.gemv!('N', T(-1), obs.xtx, β, T(1), copy!(obs.xtr, obs.xty))\n",
    "    # l2 norm of residual vector\n",
    "    updater && (obs.rtr[1] = obs.yty - dot(obs.xty, β) - dot(obs.xtr, β))\n",
    "    # assemble pieces\n",
    "    logl::T = n * log(2π) + (n - q) * log(σ²) # constant term\n",
    "    @inbounds for j in 1:q # log det term\n",
    "        logl += 2log(obs.storage_qq[j, j])\n",
    "    end\n",
    "    qf    = abs2(norm(obs.storage_q)) # quadratic form term\n",
    "    logl += (obs.rtr[1] - qf) * σ²inv \n",
    "    logl /= -2\n",
    "    ######################################\n",
    "    # TODO: Evaluate posterior mean and variance\n",
    "    ######################################    \n",
    "    # TODO\n",
    "    # get μγ, which is ΣZtΩ⁻¹r = Σ(Ztr - ZtZLU⁻¹obs.storage_q)/σ²\n",
    "    # where Ztr = Zty - ZtXβ\n",
    "    copy!(obs.storage_q2, obs.ztr)\n",
    "    # get storage_q2 = Ztr - ZtZL(L')⁻¹obs.storage_q\n",
    "    copy!(obs.μγ, obs.storage_q)\n",
    "    BLAS.trsv!('U', 'N', 'N', obs.storage_qq, obs.μγ)\n",
    "    BLAS.trmv!('L', 'N', 'N', L, obs.μγ) \n",
    "    # Put two pieces together\n",
    "    BLAS.gemv!('T', T(-1), obs.ztz, obs.μγ, T(1), obs.storage_q2)\n",
    "    BLAS.gemm!('N', 'N', T(1 / σ²), Σ, obs.storage_q2, T(0), obs.μγ)\n",
    "    #obs.μγ ./= σ²\n",
    "    \n",
    "    # νγ\n",
    "    # get (σ⁻²ztz + Σ⁻¹)⁻¹\n",
    "    copy!(obs.νγ, L)\n",
    "    LAPACK.potri!('L', obs.νγ)\n",
    "    LinearAlgebra.copytri!(obs.νγ, 'L')\n",
    "    BLAS.axpy!(T(1 / σ²), obs.ztz, obs.νγ)\n",
    "    LAPACK.potrf!('U', obs.νγ)\n",
    "    LAPACK.potri!('U', obs.νγ)\n",
    "    LinearAlgebra.copytri!(obs.νγ, 'U')\n",
    "    \n",
    "    ###################\n",
    "    # Return\n",
    "    ###################        \n",
    "    return logl\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a good idea to test correctness and efficiency of the single datum objective/posterior mean/var evaluator here. It's the same test datum in HW2 and HW4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "Random.seed!(257)\n",
    "# dimension\n",
    "n, p, q = 2000, 5, 3\n",
    "# predictors\n",
    "X = [ones(n) randn(n, p - 1)]\n",
    "Z = [ones(n) randn(n, q - 1)]\n",
    "# parameter values\n",
    "β  = [2.0; -1.0; rand(p - 2)]\n",
    "σ² = 1.5\n",
    "Σ  = fill(0.1, q, q) + 0.9I # compound symmetry \n",
    "L  = Matrix(cholesky(Symmetric(Σ)).L)\n",
    "# generate y\n",
    "y  = X * β + Z * rand(MvNormal(Σ)) + sqrt(σ²) * randn(n)\n",
    "\n",
    "# form the LmmObs object\n",
    "obs = LmmObs(y, X, Z);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logl = logl!(obs, β, Σ, L, σ², true) = -3247.4568580638243\n",
      "obs.μγ = [-1.7352999248286247, -1.2234665777053957, -0.250201904077782]\n",
      "obs.νγ = [0.0007495521480103862 4.1880268195223775e-6 8.59502834901115e-6; 4.1880268195223775e-6 0.0007599372708603274 -1.0092121486077335e-5; 8.59502834901115e-6 -1.0092121486077335e-5 0.0007370698232610101]\n"
     ]
    }
   ],
   "source": [
    "@show logl = logl!(obs, β, Σ, L, σ², true)\n",
    "@show obs.μγ\n",
    "@show obs.νγ;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will lose all 20 points if following statement throws `AssertionError`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "@assert abs(logl - (-3247.4568580638247)) < 1e-8\n",
    "@assert norm(obs.μγ - [-1.7352999248278138, \n",
    "        -1.2234665777052611, -0.25020190407767146]) < 1e-8\n",
    "@assert norm(obs.νγ - [0.0007495521482876466 4.188026899159083e-6 8.595028393969659e-6; \n",
    "        4.1880268803062436e-6 0.0007599372708508531 -1.0092121451703577e-5; \n",
    "        8.595028373480989e-6 -1.009212147054782e-5 0.0007370698230021235]) < 1e-8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Efficiency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Benchmark for efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: \n",
       "  memory estimate:  0 bytes\n",
       "  allocs estimate:  0\n",
       "  --------------\n",
       "  minimum time:     1.786 μs (0.00% GC)\n",
       "  median time:      1.814 μs (0.00% GC)\n",
       "  mean time:        1.871 μs (0.00% GC)\n",
       "  maximum time:     12.823 μs (0.00% GC)\n",
       "  --------------\n",
       "  samples:          10000\n",
       "  evals/sample:     10"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm_obj = @benchmark logl!($obs, $β, $Σ, $L, $σ², true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My median runt time is 1.7μs. You will get full credit if the median run time is within 10μs. The points you will get are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clamp(10 / (median(bm_obj).time / 1e3) * 10, 0, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check for type stability\n",
    "# @code_warntype logl!(obs, β, Σ, L, σ²)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using Profile\n",
    "\n",
    "# Profile.clear()\n",
    "# @profile for i in 1:10000; logl!(obs, β, Σ, L, σ²); end\n",
    "# Profile.print(format=:flat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4. LmmModel type\n",
    "\n",
    "We modify the `LmmModel` type in HW4 to hold all data points, model parameters, and intermediate arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LmmModel"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define a type that holds LMM model (data + parameters)\n",
    "struct LmmModel{T <: AbstractFloat}\n",
    "    # data\n",
    "    data :: Vector{LmmObs{T}}\n",
    "    # parameters\n",
    "    β      :: Vector{T}\n",
    "    Σ      :: Matrix{T}\n",
    "    L      :: Matrix{T}\n",
    "    σ²     :: Vector{T}    \n",
    "    # TODO: add whatever intermediate arrays you may want to pre-allocate\n",
    "    xty    :: Vector{T}\n",
    "    xtr    :: Vector{T}\n",
    "    ztr2   :: Vector{T}\n",
    "    xtxinv :: Matrix{T}\n",
    "    ztz2   :: Matrix{T}\n",
    "    storage_p :: Vector{T}\n",
    "    storage_q :: Vector{T}\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "    LmmModel(data::Vector{LmmObs})\n",
    "\n",
    "Create an LMM model that contains data and parameters.\n",
    "\"\"\"\n",
    "function LmmModel(obsvec::Vector{LmmObs{T}}) where T <: AbstractFloat\n",
    "    # dims\n",
    "    p      = size(obsvec[1].X, 2)\n",
    "    q      = size(obsvec[1].Z, 2)\n",
    "    # parameters\n",
    "    β      = Vector{T}(undef, p)\n",
    "    Σ      = Matrix{T}(undef, q, q)\n",
    "    L      = Matrix{T}(undef, q, q)\n",
    "    σ²     = Vector{T}(undef, 1)    \n",
    "    # intermediate arrays\n",
    "    xty    = zeros(T, p)\n",
    "    xtr    = similar(xty)\n",
    "    ztr2   = Vector{T}(undef, abs2(q))\n",
    "    xtxinv = zeros(T, p, p)\n",
    "    storage_p = similar(β)\n",
    "    storage_q = Vector{T}(undef, q)\n",
    "    # pre-calculate \\sum_i Xi^T Xi and \\sum_i Xi^T y_i\n",
    "    @inbounds for i in eachindex(obsvec)\n",
    "        obs = obsvec[i]\n",
    "        BLAS.axpy!(T(1), obs.xtx, xtxinv)\n",
    "        BLAS.axpy!(T(1), obs.xty, xty)\n",
    "    end\n",
    "    # invert X'X\n",
    "    LAPACK.potrf!('U', xtxinv)\n",
    "    LAPACK.potri!('U', xtxinv)\n",
    "    LinearAlgebra.copytri!(xtxinv, 'U')\n",
    "    ztz2   = Matrix{T}(undef, abs2(q), abs2(q))\n",
    "    LmmModel(obsvec, β, Σ, L, σ², xty, xtr, ztr2, xtxinv, ztz2, storage_p, storage_q)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5. Implement EM update\n",
    "\n",
    "Let's write the key function `update_em!` that performs one iteration of EM update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "update_em!"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    update_em!(m::LmmModel, updater::Bool = false)\n",
    "\n",
    "Perform one iteration of EM update. It returns the log-likelihood calculated \n",
    "from input `m.β`, `m.Σ`, `m.L`, and `m.σ²`. These fields are then overwritten \n",
    "by the next EM iterate. The fields `m.data[i].xtr`, `m.data[i].ztr`, and \n",
    "`m.data[i].rtr` are updated according to the resultant `m.β`. If `updater==true`, \n",
    "the function first updates `m.data[i].xtr`, `m.data[i].ztr`, and \n",
    "`m.data[i].rtr` according to `m.β`. If `updater==false`, it assumes these fields \n",
    "are pre-computed.\n",
    "\"\"\"\n",
    "function update_em!(m::LmmModel{T}, updater::Bool = false) where T <: AbstractFloat\n",
    "    logl = zero(T)\n",
    "    # TODO: update m.β\n",
    "\n",
    "    @inbounds for i in eachindex(m.data)\n",
    "        logl += logl!(m.data[i], m.β, m.Σ, m.L, m.σ²[1], updater)\n",
    "    end\n",
    "\n",
    "    fill!(m.storage_p, 0)\n",
    "    @inbounds for i in eachindex(m.data)\n",
    "        # calculate intermediate xtzμᵢ for m.β\n",
    "        BLAS.gemm!('T', 'N', T(1), m.data[i].ztx, m.data[i].μγ, T(1), m.storage_p)\n",
    "    end\n",
    "    BLAS.axpy!(T(1), m.xty, m.storage_p)\n",
    "    BLAS.gemv!('N', T(1), m.xtxinv, m.storage_p, T(1), m.β)\n",
    "\n",
    "    # TODO: update m.data[i].ztr, m.data[i].xtr, m.data[i].rtr\n",
    "    @inbounds for i in eachindex(m.data)\n",
    "        obs = m.data[i]\n",
    "        BLAS.gemv!('N', T(-1), obs.ztx, m.β, T(1), copy!(obs.ztr, obs.zty)) # O(pq)\n",
    "        BLAS.gemv!('N', T(-1), obs.xtx, m.β, T(1), copy!(obs.xtr, obs.xty))\n",
    "        obs.rtr[1] = obs.yty - dot(obs.xty, m.β) - dot(obs.xtr, m.β)\n",
    "    end\n",
    "\n",
    "    # TODO: update m.σ²\n",
    "    # calculate intermediate for m.σ²\n",
    "    fill!(m.σ², 0)\n",
    "    #fill!(m.storage_q, 0)\n",
    "    fill!(m.Σ, 0)\n",
    "    mytemp = 0\n",
    "    @inbounds for i in eachindex(m.data)\n",
    "        obs = m.data[i]\n",
    "        # -rtr\n",
    "        m.σ²[1] -= obs.rtr[1]\n",
    "        # - tr(ztzVi)\n",
    "        m.σ²[1] -= dot(obs.ztz, obs.νγ)\n",
    "        # - μ'ztzμ + 2rtzμ\n",
    "        BLAS.gemv!('N', T(-1), obs.ztz, obs.μγ, T(0), m.storage_q)\n",
    "        BLAS.axpy!(T(2), obs.ztr, m.storage_q)\n",
    "        m.σ²[1] += dot(m.storage_q, obs.μγ)\n",
    "        mytemp += size(obs.X, 1)\n",
    "\n",
    "        # update m.Σ\n",
    "        BLAS.axpy!(T(1), obs.νγ, m.Σ) \n",
    "        BLAS.gemm!('N', 'T', T(1), obs.μγ, obs.μγ, T(1), m.Σ)\n",
    "    end\n",
    "    m.σ²[1] /= mytemp\n",
    "    \n",
    "    m.Σ ./= length(m.data)\n",
    "    copy!(m.L, m.Σ)\n",
    "    LAPACK.potrf!('L', m.L)\n",
    "    # update m.Σ and m.L\n",
    "\n",
    "    # return log-likelihood at input parameter values\n",
    "    # update the llikelihood and expectation and var\n",
    "    #@inbounds for i in eachindex(m.data)\n",
    "    #    logl += logl!(m.data[i], m.β, m.Σ, m.L, m.σ²[1], false)\n",
    "    #end\n",
    "\n",
    "    logl\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "length(lmm.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q6. (30 pts) Test data\n",
    "\n",
    "Let's generate a fake longitudinal data set (same as HW4) to test our algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "Random.seed!(257)\n",
    "\n",
    "# dimension\n",
    "m      = 1000 # number of individuals\n",
    "ns     = rand(1500:2000, m) # numbers of observations per individual\n",
    "p      = 5 # number of fixed effects, including intercept\n",
    "q      = 3 # number of random effects, including intercept\n",
    "obsvec = Vector{LmmObs{Float64}}(undef, m)\n",
    "# true parameter values\n",
    "βtrue  = [0.1; 6.5; -3.5; 1.0; 5]\n",
    "σ²true = 1.5\n",
    "σtrue  = sqrt(σ²true)\n",
    "Σtrue  = Matrix(Diagonal([2.0; 1.2; 1.0]))\n",
    "Ltrue  = Matrix(cholesky(Symmetric(Σtrue)).L)\n",
    "# generate data\n",
    "for i in 1:m\n",
    "    # first column intercept, remaining entries iid std normal\n",
    "    X = Matrix{Float64}(undef, ns[i], p)\n",
    "    X[:, 1] .= 1\n",
    "    @views Distributions.rand!(Normal(), X[:, 2:p])\n",
    "    # first column intercept, remaining entries iid std normal\n",
    "    Z = Matrix{Float64}(undef, ns[i], q)\n",
    "    Z[:, 1] .= 1\n",
    "    @views Distributions.rand!(Normal(), Z[:, 2:q])\n",
    "    # generate y\n",
    "    y = X * βtrue .+ Z * (Ltrue * randn(q)) .+ σtrue * randn(ns[i])\n",
    "    # form a LmmObs instance\n",
    "    obsvec[i] = LmmObs(y, X, Z)\n",
    "end\n",
    "# form a LmmModel instance\n",
    "lmm = LmmModel(obsvec);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correctness\n",
    "\n",
    "Evaluate log-likelihood and gradient at the true parameter values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obj1 = update_em!(lmm, true) = -2.854712648683302e6\n",
      "lmm.β = [0.17516468822946119, 12.997336891639561, -6.995312473651152, 1.9938647337839266, 10.001935826628866]\n",
      "lmm.Σ = [2.057126019116299 -0.010005181563170231 0.018147868730423535; -0.010005181563170231 1.182311505922221 -0.02266793247403069; 0.018147868730423535 -0.02266793247403069 0.9380104634208056]\n",
      "lmm.L = [1.4342684613126997 -0.010005181563170231 0.018147868730423535; -0.006975808109182774 1.0873191086444884 -0.02266793247403069; 0.01265304872827914 -0.020766366611780718 0.9682040703263081]\n",
      "lmm.σ² = [4.197892584331108]\n",
      "\n",
      "obj2 = update_em!(lmm, false) = -1.9974805736498233e7\n",
      "lmm.β = [0.1702361330652321, 19.483082952976343, -10.484571465338012, 2.9859910085827464, 14.995775196599809]\n",
      "lmm.Σ = [2.132647449140059 -0.01656923664485023 0.009297193982249253; -0.01656923664485023 1.2361383997614228 -0.026533208555677163; 0.009297193982249253 -0.026533208555677163 0.9821380331497137]\n",
      "lmm.L = [1.4603586714023575 -0.01656923664485023 0.009297193982249253; -0.011346004902301893 1.1117597168157243 -0.026533208555677163; 0.006366377085515105 -0.02380098434025286 0.9907224715009516]\n",
      "lmm.σ² = [4.643234385018095]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1-element Array{Float64,1}:\n",
       " 4.643234385018095"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "copy!(lmm.β, βtrue)\n",
    "copy!(lmm.Σ, Σtrue)\n",
    "copy!(lmm.L, Ltrue)\n",
    "lmm.σ²[1] = σ²true\n",
    "@show obj1 = update_em!(lmm, true)\n",
    "@show lmm.β\n",
    "@show lmm.Σ\n",
    "@show lmm.L\n",
    "@show lmm.σ²\n",
    "println()\n",
    "@show obj2 = update_em!(lmm, false)\n",
    "@show lmm.β\n",
    "@show lmm.Σ\n",
    "@show lmm.L\n",
    "@show lmm.σ²"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test correctness. You will loss all 30 points if following code throws `AssertError`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "AssertionError: abs(obj1 - -2.854712648683302e6) < 1.0e-6",
     "output_type": "error",
     "traceback": [
      "AssertionError: abs(obj1 - -2.854712648683302e6) < 1.0e-6",
      "",
      "Stacktrace:",
      " [1] top-level scope at In[10]:1"
     ]
    }
   ],
   "source": [
    "@assert abs(obj1 - (-2.854712648683302e6)) < 1e-6\n",
    "@assert abs(obj2 - (-2.8547098610621705e6)) < 1e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Efficiency\n",
    "\n",
    "Test efficiency of EM update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm_emupdate = @benchmark update_em!($lmm, true) setup=(\n",
    "    copy!(lmm.β, βtrue);\n",
    "    copy!(lmm.Σ, Σtrue);\n",
    "    copy!(lmm.L, Ltrue);\n",
    "    lmm.σ²[1] = σ²true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My median run time is 2.17ms. You will get full credit if your median run time is within 10ms. The points you will get are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clamp(10 / (median(bm_emupdate).time / 1e6) * 10, 0, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory\n",
    "\n",
    "You will lose 1 point for each 100 bytes memory allocation. So the points you will get is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clamp(10 - median(bm_emupdate).memory / 100, 0, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q7. Starting point\n",
    "\n",
    "We use the same least squares estimates as in HW4 as starting point. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    init_ls!(m::LmmModel)\n",
    "\n",
    "Initialize parameters of a `LmmModel` object from the least squares estimate. \n",
    "`m.β`, `m.L`, and `m.σ²` are overwritten with the least squares estimates.\n",
    "\"\"\"\n",
    "function init_ls!(m::LmmModel{T}) where T <: AbstractFloat\n",
    "    p, q = size(m.data[1].X, 2), size(m.data[1].Z, 2)\n",
    "    # LS estimate for β\n",
    "    mul!(m.β, m.xtxinv, m.xty)\n",
    "    # LS etimate for σ2 and Σ\n",
    "    rss, ntotal = zero(T), 0\n",
    "    fill!(m.ztz2, 0)\n",
    "    fill!(m.ztr2, 0)    \n",
    "    @inbounds for i in eachindex(m.data)\n",
    "        obs = m.data[i]\n",
    "        ntotal += length(obs.y)\n",
    "        # update Xt * res\n",
    "        BLAS.gemv!('N', T(-1), obs.xtx, m.β, T(1), copy!(obs.xtr, obs.xty))\n",
    "        # rss of i-th individual\n",
    "        rss += obs.yty - dot(obs.xty, m.β) - dot(obs.xtr, m.β)\n",
    "        # update Zi' * res\n",
    "        BLAS.gemv!('N', T(-1), obs.ztx, m.β, T(1), copy!(obs.ztr, obs.zty))\n",
    "        # Zi'Zi ⊗ Zi'Zi\n",
    "        kron_axpy!(obs.ztz, obs.ztz, m.ztz2)\n",
    "        # Zi'res ⊗ Zi'res\n",
    "        kron_axpy!(obs.ztr, obs.ztr, m.ztr2)\n",
    "    end\n",
    "    m.σ²[1] = rss / ntotal\n",
    "    # LS estimate for Σ = LLt\n",
    "    LAPACK.potrf!('U', m.ztz2)\n",
    "    BLAS.trsv!('U', 'T', 'N', m.ztz2, m.ztr2)\n",
    "    BLAS.trsv!('U', 'N', 'N', m.ztz2, m.ztr2)\n",
    "    copyto!(m.Σ, m.ztr2)\n",
    "    copy!(m.L, m.Σ)\n",
    "    LAPACK.potrf!('L', m.L)\n",
    "    for j in 2:q, i in 1:j-1\n",
    "        m.L[i, j] = 0\n",
    "    end\n",
    "    m\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "    kron_axpy!(A, X, Y)\n",
    "\n",
    "Overwrite `Y` with `A ⊗ X + Y`. Same as `Y += kron(A, X)` but\n",
    "more memory efficient.\n",
    "\"\"\"\n",
    "function kron_axpy!(\n",
    "        A::AbstractVecOrMat{T},\n",
    "        X::AbstractVecOrMat{T},\n",
    "        Y::AbstractVecOrMat{T}\n",
    "        ) where T <: Real\n",
    "    m, n = size(A, 1), size(A, 2)\n",
    "    p, q = size(X, 1), size(X, 2)\n",
    "    @assert size(Y, 1) == m * p\n",
    "    @assert size(Y, 2) == n * q\n",
    "    @inbounds for j in 1:n\n",
    "        coffset = (j - 1) * q\n",
    "        for i in 1:m\n",
    "            a = A[i, j]\n",
    "            roffset = (i - 1) * p            \n",
    "            for l in 1:q\n",
    "                r = roffset + 1\n",
    "                c = coffset + l\n",
    "                for k in 1:p                \n",
    "                    Y[r, c] += a * X[k, l]\n",
    "                    r += 1\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    Y\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_ls!(lmm)\n",
    "@show lmm.β\n",
    "@show lmm.Σ\n",
    "@show lmm.L\n",
    "@show lmm.σ²"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q8. Estimation by EM\n",
    "\n",
    "We write a function `fit!` that implements the EM algorithm for estimating LMM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    fit!(m::LmmModel)\n",
    "\n",
    "Fit an `LmmModel` object by MLE using a EM algorithm. Start point \n",
    "should be provided in `m.β`, `m.σ²`, `m.L`.\n",
    "\"\"\"\n",
    "function fit!(\n",
    "        m       :: LmmModel;\n",
    "        maxiter :: Integer       = 10_000,\n",
    "        ftolrel :: AbstractFloat = 1e-12,\n",
    "        prtfreq :: Integer       = 0\n",
    "    )\n",
    "    obj = update_em!(m, true)\n",
    "    for iter in 0:maxiter\n",
    "        obj_old = obj\n",
    "        # EM update\n",
    "        obj = update_em!(m, false)\n",
    "        # print obj\n",
    "        prtfreq > 0 && rem(iter, prtfreq) == 0 && println(\"iter=$iter, obj=$obj\")\n",
    "        # check monotonicity\n",
    "        obj < obj_old && (@warn \"monotoniciy violated\")\n",
    "        # check convergence criterion\n",
    "        (obj - obj_old) < ftolrel * (abs(obj_old) + 1) && break\n",
    "        # warning about non-convergence\n",
    "        iter == maxiter && (@warn \"maximum iterations reached\")\n",
    "    end\n",
    "    m\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q9. (20 pts) Test drive\n",
    "\n",
    "Now we can run our EM algorithm to compute the MLE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize from least squares\n",
    "init_ls!(lmm)\n",
    "\n",
    "@time fit!(lmm, prtfreq = 100);\n",
    "\n",
    "println(\"objective value at solution: \", update_em!(lmm)); println()\n",
    "println(\"solution values:\")\n",
    "@show lmm.β\n",
    "@show lmm.σ²\n",
    "@show lmm.L * transpose(lmm.L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correctness\n",
    "\n",
    "You get 10 points if the following code does not throw `AssertError`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# objective at solution should be close enough to the optimal\n",
    "@assert update_em!(lmm) > -2.85471e6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Efficiency\n",
    "\n",
    "My median run time 1.8s. You get 10 points if your median run time is within 5s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm_em = @benchmark fit!($lmm) setup = (init_ls!(lmm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the points you get\n",
    "clamp(5 / (median(bm_em).time / 1e9) * 10, 0, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q10. (10 pts) EM vs Newton type algorithms\n",
    "\n",
    "Contrast EM algorithm to the Newton type algorithms (gradient free, gradient based, using Hessian) in HW4, in terms of the stability, convergence rate (how fast the algorithm is converging),  final objective value, total run time, derivation, and implementation efforts. "
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.4.2",
   "language": "julia",
   "name": "julia-1.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.4.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "87px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
