{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Biostat 257 HW2\n",
    "### Caesar Z. Li\n",
    "### UID: 704135662\n",
    "\n",
    "## Q1. (Optional, 30 bonus pts) Derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load necessary packages; make sure install them first\n",
    "using BenchmarkTools, CSV, DataFrames, DelimitedFiles, Distributions\n",
    "using Ipopt, LinearAlgebra, MathProgBase, MixedModels, NLopt\n",
    "using Random, RCall, Revise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2. (20 pts) Objective and gradient evaluator for a single datum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a type that holds an LMM datum\n",
    "struct LmmObs{T <: AbstractFloat}\n",
    "    # data\n",
    "    y          :: Vector{T}\n",
    "    X          :: Matrix{T}\n",
    "    Z          :: Matrix{T}\n",
    "    # arrays for holding gradient\n",
    "    ∇β         :: Vector{T}\n",
    "    ∇σ²        :: Vector{T}\n",
    "    ∇Σ         :: Matrix{T}    \n",
    "    # working arrays\n",
    "    # TODO: whatever intermediate arrays you may want to pre-allocate\n",
    "    yty        :: T\n",
    "    xty        :: Vector{T}\n",
    "    zty        :: Vector{T}\n",
    "    storage_p  :: Vector{T}\n",
    "    storage_q  :: Vector{T}\n",
    "    storage_q2 :: Vector{T}\n",
    "    xtx        :: Matrix{T}\n",
    "    ztx        :: Matrix{T}\n",
    "    ztz        :: Matrix{T}\n",
    "    storage_qq :: Matrix{T}\n",
    "    storage_qq2 :: Matrix{T}\n",
    "    storage_qq3 :: Matrix{T}\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "logl!"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "    LmmObs(y::Vector, X::Matrix, Z::Matrix)\n",
    "\n",
    "Create an LMM datum of type `LmmObs`.\n",
    "\"\"\"\n",
    "function LmmObs(\n",
    "        y::Vector{T}, \n",
    "        X::Matrix{T}, \n",
    "        Z::Matrix{T}\n",
    "    ) where T <: AbstractFloat\n",
    "    n, p, q    = size(X, 1), size(X, 2), size(Z, 2)    \n",
    "    ∇β         = Vector{T}(undef, p)\n",
    "    ∇σ²        = Vector{T}(undef, 1)\n",
    "    ∇Σ         = Matrix{T}(undef, q, q)    \n",
    "    yty        = abs2(norm(y))\n",
    "    xty        = transpose(X) * y\n",
    "    zty        = transpose(Z) * y    \n",
    "    storage_p  = Vector{T}(undef, p)\n",
    "    storage_q  = Vector{T}(undef, q)\n",
    "    storage_q2 = Vector{T}(undef, q)\n",
    "    xtx        = transpose(X) * X\n",
    "    ztx        = transpose(Z) * X\n",
    "    ztz        = transpose(Z) * Z\n",
    "    storage_qq = similar(ztz)\n",
    "    storage_qq2 = similar(ztz)\n",
    "    storage_qq3 = similar(ztz)\n",
    "    LmmObs(y, X, Z, ∇β, ∇σ², ∇Σ, \n",
    "        yty, xty, zty, storage_p, storage_q, storage_q2, \n",
    "        xtx, ztx, ztz, storage_qq, storage_qq2, storage_qq3)\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "    logl!(obs::LmmObs, β, L, σ², needgrad=false)\n",
    "\n",
    "Evaluate the log-likelihood of a single LMM datum at parameter values `β`, `L`, \n",
    "and `σ²`. If `needgrad==true`, then `obs.∇β`, `obs.∇Σ`, and `obs.σ² are filled \n",
    "with the corresponding gradient.\n",
    "\"\"\"\n",
    "function logl!(\n",
    "        obs      :: LmmObs{T}, \n",
    "        β        :: Vector{T}, \n",
    "        L        :: Matrix{T}, \n",
    "        σ²       :: T,\n",
    "        needgrad :: Bool = true\n",
    "    ) where T <: AbstractFloat\n",
    "    n, p, q = size(obs.X, 1), size(obs.X, 2), size(obs.Z, 2)\n",
    "    ####################\n",
    "    # Evaluate objective\n",
    "    ####################    \n",
    "    # form the q-by-q matrix: M = σ² * I + Lt Zt Z L\n",
    "     copy!(obs.storage_qq, obs.ztz)\n",
    "    BLAS.trmm!('L', 'L', 'T', 'N', T(1), L, obs.storage_qq) # O(q^3)\n",
    "    BLAS.trmm!('R', 'L', 'N', 'N', T(1), L, obs.storage_qq) # O(q^3)\n",
    "    @inbounds for j in 1:q\n",
    "        obs.storage_qq[j, j] += σ²\n",
    "    end\n",
    "    # cholesky on M = σ² * I + Lt Zt Z L\n",
    "    LAPACK.potrf!('U', obs.storage_qq) # O(q^3)\n",
    "    # storage_q = (Mchol.U') \\ (Lt * (Zt * res))\n",
    "    BLAS.gemv!('N', T(-1), obs.ztx, β, T(1), copy!(obs.storage_q, obs.zty)) # O(pq)\n",
    "    BLAS.trmv!('L', 'T', 'N', L, obs.storage_q)    # O(q^2)\n",
    "    BLAS.trsv!('U', 'T', 'N', obs.storage_qq, obs.storage_q) # O(q^3)\n",
    "    # l2 norm of residual vector\n",
    "    copy!(obs.storage_p, obs.xty)\n",
    "    rtr  = obs.yty +\n",
    "        dot(β, BLAS.gemv!('N', T(1), obs.xtx, β, T(-2), obs.storage_p))\n",
    "    # assemble pieces\n",
    "    logl::T = n * log(2π) + (n - q) * log(σ²) # constant term\n",
    "    @inbounds for j in 1:q\n",
    "        logl += 2log(obs.storage_qq[j, j])\n",
    "    end\n",
    "    qf    = abs2(norm(obs.storage_q)) # quadratic form term\n",
    "    logl += (rtr - qf) / σ² \n",
    "    logl /= -2\n",
    "    ###################\n",
    "    # Evaluate gradient\n",
    "    ###################    \n",
    "    if needgrad\n",
    "        # TODO: fill ∇β, ∇L, ∇σ² by gradients\n",
    "        # form ∇β first:\n",
    "        # fill in ∇β with Xt(y - Xβ) = xty - XtXβ\n",
    "        copy!(obs.∇β, obs.xty)\n",
    "        BLAS.gemv!('N', T(-1), obs.xtx, β, T(1), obs.∇β)\n",
    "        # storage_q2 = (Mchol.U) \\ ( (Mchol.U') \\ (Lt * (Zt * res)) )\n",
    "        copy!(obs.storage_q2, obs.storage_q)\n",
    "        BLAS.trsv!('U', 'N', 'N', obs.storage_qq, obs.storage_q2)\n",
    "        BLAS.trmv!('L', 'N', 'N', L, obs.storage_q2) \n",
    "        # Put two pieces together, while makeing storage_q2 .= XtZ * storage_q2\n",
    "        BLAS.gemv!('T', T(-1), obs.ztx, obs.storage_q2, T(1), obs.∇β)\n",
    "        obs.∇β ./= σ²\n",
    "        \n",
    "        # form ∇σ²:\n",
    "        # expand rtΩ⁻²r into rtr + ( rt * ZLM⁻¹ Lt ) * ZtZ * ( LM⁻¹LtZt * r ) +\n",
    "        #  rt * ZLM⁻¹ LtZt * r \n",
    "        # Note the third term is already computed as 'qf' in earlier code\n",
    "        # rtr is computed as well, so I need only compute the second term.\n",
    "        BLAS.gemv!('N', T(1), obs.ztz, obs.storage_q2, T(0), obs.storage_q)\n",
    "        obs.∇σ² .= dot(obs.storage_q2, obs.storage_q)\n",
    "        obs.∇σ² .+= rtr - 2qf\n",
    "        obs.∇σ² ./= σ²\n",
    "        # minus tr(Ω⁻¹)\n",
    "        obs.storage_qq2 .= inv(UpperTriangular(obs.storage_qq))\n",
    "        copy!(obs.storage_qq3, obs.ztz)\n",
    "        BLAS.trmm!('L', 'L', 'T', 'N', T(1), L, obs.storage_qq3)\n",
    "        BLAS.trmm!('R', 'L', 'N', 'N', T(1), L, obs.storage_qq3)\n",
    "        BLAS.trmm!('R', 'U', 'N', 'N', T(1), obs.storage_qq2, obs.storage_qq3)\n",
    "        obs.∇σ² .+= dot(obs.storage_qq2, obs.storage_qq3)\n",
    "        obs.∇σ² .-= n \n",
    "        obs.∇σ² ./= (2 * σ²)\n",
    "        \n",
    "        # form ∇Σ:\n",
    "        # first part, get (-ZtZ + ZtZ * LM⁻¹Lt * ZtZ) L\n",
    "        # put first half in obs.storage_qq2\n",
    "        copy!(obs.storage_qq3, L)\n",
    "        BLAS.trmm!('R', 'U', 'N', 'N', T(1), obs.storage_qq2, obs.storage_qq3)\n",
    "        BLAS.gemm!('N', 'T', T(1), obs.storage_qq3, obs.storage_qq3, T(0), obs.∇Σ)\n",
    "        BLAS.gemm!('N', 'N', T(1), obs.ztz, obs.∇Σ, T(0), obs.storage_qq3)\n",
    "        @inbounds for j in 1:q\n",
    "             obs.storage_qq3[j, j] -= 1.0\n",
    "        end\n",
    "        BLAS.gemm!('N', 'N', T(1), obs.storage_qq3, obs.ztz, T(0), obs.∇Σ)\n",
    "        BLAS.trmm!('R', 'L', 'N', 'N', T(1), L, obs.∇Σ)\n",
    "        \n",
    "        #BLAS.gemm!('N', 'N', T(1), obs.ztz, L, T(0), obs.storage_qq3)\n",
    "        #BLAS.gemm!('N', 'N', T(1), obs.storage_qq3, obs.storage_qq2, T(0), obs.∇Σ)\n",
    "        #BLAS.gemm!('N', 'T', T(1), obs.∇Σ, obs.∇Σ, T(0), obs.storage_qq3)\n",
    "        #BLAS.gemm!('N', 'N', T(1), obs.storage_qq3, L, T(0), obs.∇Σ)       \n",
    "        #BLAS.gemm!('N', 'N', T(- 1), obs.ztz, L, T(1), obs.∇Σ)\n",
    "        \n",
    "        # second part, use Ω⁻¹r from earlier to get ZΩ⁻¹r \n",
    "        copy!(obs.storage_q, obs.zty)\n",
    "        BLAS.gemv!('N', T(- 1), obs.ztx, β, T(1), obs.storage_q)\n",
    "        # we have storage_q2 = L(Mchol.U) \\ ( (Mchol.U') \\ (Lt * (Zt * res)) )\n",
    "        # Put two pieces together, only this time storage_q2 .= ZtZ * storage_q2\n",
    "        BLAS.gemv!('N', T(- 1), obs.ztz, obs.storage_q2, T(1), obs.storage_q)\n",
    "        # outer product this vector, then * L\n",
    "        BLAS.gemm!('N', 'T', T(1), obs.storage_q, obs.storage_q, T(0), obs.storage_qq)\n",
    "        BLAS.gemm!('N', 'N', T(1 / σ²), obs.storage_qq, L, T(1), obs.∇Σ)\n",
    "        obs.∇Σ ./= σ²\n",
    "        \n",
    "        #sleep(1e-3) # pretend this step takes 1ms\n",
    "    end    \n",
    "    ###################\n",
    "    # Return\n",
    "    ###################        \n",
    "    return logl    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((obs.xty .- obs.xtx * β) - (obs.ztx)' * L * inv(obs.storage_qq) * obs.storage_q) / σ² = [-234.55783448051343, 23.315710964270618, 209.356779599957, 132.02138431384148, -9.009747216169743]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5-element Array{Float64,1}:\n",
       " -234.55783448051343 \n",
       "   23.315710964270618\n",
       "  209.356779599957   \n",
       "  132.02138431384148 \n",
       "   -9.009747216169743"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@show (obs.xty .- obs.xtx * β - obs.ztx' * L * inv(obs.storage_qq) * obs.storage_q )/σ²"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×3 Array{Float64,2}:\n",
       "  1.83101     1.82142     0.0607169\n",
       "  1.84331     0.116693    0.0714718\n",
       " -0.0332479  -0.0202561  -1.00808  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = - Z' * (inv(σ² * I + Z * Σ * Z')) * Z * L +\n",
    " Z' * (inv(σ² * I + Z * Σ * Z')) * (y - X * β) * (y - X * β)' * (inv(σ² * I + Z * Σ * Z')) * Z * L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×3 Array{Float64,2}:\n",
       "  6.67385   3.592     -0.15898 \n",
       "  3.592     3.41652   -0.135699\n",
       " -0.15898  -0.135699   1.01775 "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a * a'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×3 Array{Float64,2}:\n",
       "  2.66011     1.73256    -0.0311919 \n",
       "  1.73256     1.12844    -0.0203157 \n",
       " -0.0311919  -0.0203157   0.00036575"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs.storage_qq / σ²^2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×3 Array{Float64,2}:\n",
       " -0.999238     0.100362     0.0916239\n",
       " -6.59332e-5  -1.00425      0.0916019\n",
       " -6.10366e-5  -7.53052e-5  -1.00845  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " - Z' * (I - Z * L * obs.storage_qq2 * obs.storage_qq2' * L' * Z') * Z * L / σ²"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a good idea to test correctness and efficiency of the single datum objective/gradient evaluator here. First generate the same data set as in HW2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Random.seed!(257)\n",
    "# dimension\n",
    "n, p, q = 2000, 5, 3\n",
    "# predictors\n",
    "X  = [ones(n) randn(n, p - 1)]\n",
    "Z  = [ones(n) randn(n, q - 1)]\n",
    "# parameter values\n",
    "β  = [2.0; -1.0; rand(p - 2)]\n",
    "σ² = 1.5\n",
    "Σ  = fill(0.1, q, q) + 0.9I # compound symmetry \n",
    "L  = Matrix(cholesky(Symmetric(Σ)).L)\n",
    "# generate y\n",
    "y  = X * β + Z * rand(MvNormal(Σ)) + sqrt(σ²) * randn(n)\n",
    "\n",
    "# form the LmmObs object\n",
    "obs = LmmObs(y, X, Z);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1  Correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logl = logl!(obs, β, L, σ², true) = -3247.4568580638256\n",
      "obs.∇β = [-1.6309843232714532, -77.527515580418, -14.702372116010645, 6.978485518990293, -57.711823176822044]\n",
      "obs.∇σ² = [-4.820377758258701]\n",
      "obs.∇Σ = [1.8310092226973647 1.8214186972659292 0.06071688596771451; 1.843309519464353 0.11669335312204465 0.07147176879465071; -0.03324793384975084 -0.020256067839013816 -1.0080835623768392]\n"
     ]
    }
   ],
   "source": [
    "@show logl = logl!(obs, β, L, σ², true)\n",
    "@show obs.∇β\n",
    "@show obs.∇σ²\n",
    "@show obs.∇Σ;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will lose all 20 points if following statement throws `AssertionError`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "AssertionError: norm(obs.∇Σ - [1.6423791649290531 1.82502407722348 0.06127650043330721; 1.82502407722348 0.1107239137055005 0.07213050869971993; 0.06127650043330721 0.07213050869971993 -1.0173748515299939]) < 1.0e-8",
     "output_type": "error",
     "traceback": [
      "AssertionError: norm(obs.∇Σ - [1.6423791649290531 1.82502407722348 0.06127650043330721; 1.82502407722348 0.1107239137055005 0.07213050869971993; 0.06127650043330721 0.07213050869971993 -1.0173748515299939]) < 1.0e-8",
      "",
      "Stacktrace:",
      " [1] top-level scope at In[218]:4"
     ]
    }
   ],
   "source": [
    "@assert abs(logl - (-3247.4568580638247)) < 1e-8\n",
    "@assert norm(obs.∇β - [-1.63098432327115, -77.52751558041871, -14.70237211601065, \n",
    "        6.978485518989568, -57.71182317682199]) < 1e-8\n",
    "@assert norm(obs.∇Σ - [1.6423791649290531 1.82502407722348 0.06127650043330721; \n",
    "        1.82502407722348 0.1107239137055005 0.07213050869971993; \n",
    "        0.06127650043330721 0.07213050869971993 -1.0173748515299939]) < 1e-8\n",
    "@assert abs(obs.∇σ²[1] - (-4.8203777582588145)) < 1e-8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2  Efficiency\n",
    "Benchmark for evaluating objective only. This is what we did in HW2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: \n",
       "  memory estimate:  0 bytes\n",
       "  allocs estimate:  0\n",
       "  --------------\n",
       "  minimum time:     1.786 μs (0.00% GC)\n",
       "  median time:      1.806 μs (0.00% GC)\n",
       "  mean time:        1.811 μs (0.00% GC)\n",
       "  maximum time:     4.995 μs (0.00% GC)\n",
       "  --------------\n",
       "  samples:          10000\n",
       "  evals/sample:     10"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@benchmark logl!($obs, $β, $L, $σ², false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: \n",
       "  memory estimate:  176 bytes\n",
       "  allocs estimate:  2\n",
       "  --------------\n",
       "  minimum time:     7.287 μs (0.00% GC)\n",
       "  median time:      7.359 μs (0.00% GC)\n",
       "  mean time:        7.515 μs (0.00% GC)\n",
       "  maximum time:     20.135 μs (0.00% GC)\n",
       "  --------------\n",
       "  samples:          10000\n",
       "  evals/sample:     4"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm_objgrad = @benchmark logl!($obs, $β, $L, $σ², true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My median runt time is 2.1μs. You will get full credit (10 pts) if the median run time is within 10μs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.0"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  The points you will get are\n",
    "clamp(10 / (median(bm_objgrad).time / 1e3) * 10, 0, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3. LmmModel type\n",
    "We create a `LmmModel` type to hold all data points and model parameters. Log-likelihood/gradient of a LmmModel object is simply the sum of log-likelihood/gradient of individual data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a type that holds LMM model (data + parameters)\n",
    "struct LmmModel{T <: AbstractFloat} <: MathProgBase.AbstractNLPEvaluator\n",
    "    # data\n",
    "    data :: Vector{LmmObs{T}}\n",
    "    # parameters\n",
    "    β    :: Vector{T}\n",
    "    L    :: Matrix{T}\n",
    "    σ²   :: Vector{T}    \n",
    "    # arrays for holding gradient\n",
    "    ∇β   :: Vector{T}\n",
    "    ∇σ²  :: Vector{T}\n",
    "    ∇L   :: Matrix{T}\n",
    "    # TODO: add whatever intermediate arrays you may want to pre-allocate\n",
    "    xty  :: Vector{T}\n",
    "    ztr2 :: Vector{T}\n",
    "    xtx  :: Matrix{T}\n",
    "    ztz2 :: Matrix{T}\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "    LmmModel(data::Vector{LmmObs})\n",
    "\n",
    "Create an LMM model that contains data and parameters.\n",
    "\"\"\"\n",
    "function LmmModel(obsvec::Vector{LmmObs{T}}) where T <: AbstractFloat\n",
    "    # dims\n",
    "    p    = size(obsvec[1].X, 2)\n",
    "    q    = size(obsvec[1].Z, 2)\n",
    "    # parameters\n",
    "    β    = Vector{T}(undef, p)\n",
    "    L    = Matrix{T}(undef, q, q)\n",
    "    σ²   = Vector{T}(undef, 1)    \n",
    "    # gradients\n",
    "    ∇β   = similar(β)    \n",
    "    ∇σ²  = similar(σ²)\n",
    "    ∇L   = similar(L)\n",
    "    # intermediate arrays\n",
    "    xty  = Vector{T}(undef, p)\n",
    "    ztr2 = Vector{T}(undef, abs2(q))\n",
    "    xtx  = Matrix{T}(undef, p, p)\n",
    "    ztz2 = Matrix{T}(undef, abs2(q), abs2(q))\n",
    "    LmmModel(obsvec, β, L, σ², ∇β, ∇σ², ∇L, xty, ztr2, xtx, ztz2)\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "    logl!(m::LmmModel, needgrad=false)\n",
    "\n",
    "Evaluate the log-likelihood of an LMM model at parameter values `m.β`, `m.L`, \n",
    "and `m.σ²`. If `needgrad==true`, then `m.∇β`, `m.∇Σ`, and `m.σ² are filled \n",
    "with the corresponding gradient.\n",
    "\"\"\"\n",
    "function logl!(m::LmmModel{T}, needgrad::Bool = false) where T <: AbstractFloat\n",
    "    logl = zero(T)\n",
    "    if needgrad\n",
    "        fill!(m.∇β , 0)\n",
    "        fill!(m.∇L , 0)\n",
    "        fill!(m.∇σ², 0)        \n",
    "    end\n",
    "    @inbounds for i in 1:length(m.data)\n",
    "        obs = m.data[i]\n",
    "        logl += logl!(obs, m.β, m.L, m.σ²[1], needgrad)\n",
    "        if needgrad\n",
    "            BLAS.axpy!(T(1), obs.∇β, m.∇β)\n",
    "            BLAS.axpy!(T(1), obs.∇Σ, m.∇L)\n",
    "            m.∇σ²[1] += obs.∇σ²[1]\n",
    "        end\n",
    "    end\n",
    "    # obtain gradient wrt L: m.∇L = m.∇L * L\n",
    "    if needgrad\n",
    "       # TODO \n",
    "    end\n",
    "    logl\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4. (20 pts) Test data\n",
    "Let's generate a fake longitudinal data set to test our algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Random.seed!(257)\n",
    "\n",
    "# dimension\n",
    "m      = 1000 # number of individuals\n",
    "ns     = rand(1500:2000, m) # numbers of observations per individual\n",
    "p      = 5 # number of fixed effects, including intercept\n",
    "q      = 3 # number of random effects, including intercept\n",
    "obsvec = Vector{LmmObs{Float64}}(undef, m)\n",
    "# true parameter values\n",
    "βtrue  = [0.1; 6.5; -3.5; 1.0; 5]\n",
    "σ²true = 1.5\n",
    "σtrue  = sqrt(σ²true)\n",
    "Σtrue  = Matrix(Diagonal([2.0; 1.2; 1.0]))\n",
    "Ltrue  = Matrix(cholesky(Symmetric(Σtrue)).L)\n",
    "# generate data\n",
    "for i in 1:m\n",
    "    # first column intercept, remaining entries iid std normal\n",
    "    X = Matrix{Float64}(undef, ns[i], p)\n",
    "    X[:, 1] .= 1\n",
    "    @views Distributions.rand!(Normal(), X[:, 2:p])\n",
    "    # first column intercept, remaining entries iid std normal\n",
    "    Z = Matrix{Float64}(undef, ns[i], q)\n",
    "    Z[:, 1] .= 1\n",
    "    @views Distributions.rand!(Normal(), Z[:, 2:q])\n",
    "    # generate y\n",
    "    y = X * βtrue .+ Z * (Ltrue * randn(q)) .+ σtrue * randn(ns[i])\n",
    "    # form a LmmObs instance\n",
    "    obsvec[i] = LmmObs(y, X, Z)\n",
    "end\n",
    "# form a LmmModel instance\n",
    "lmm = LmmModel(obsvec);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For later comparison with other software, we save the data into a text file lmm_data.txt. Do not put this file in Git. It takes 246.6MB storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(isfile(\"lmm_data.txt\") && filesize(\"lmm_data.txt\") == 246638945) || \n",
    "open(\"lmm_data.txt\", \"w\") do io\n",
    "    p = size(lmm.data[1].X, 2)\n",
    "    q = size(lmm.data[1].Z, 2)\n",
    "    # print header\n",
    "    print(io, \"ID,Y,\")\n",
    "    for j in 1:(p-1)\n",
    "        print(io, \"X\" * string(j) * \",\")\n",
    "    end\n",
    "    for j in 1:(q-1)\n",
    "        print(io, \"Z\" * string(j) * (j < q-1 ? \",\" : \"\\n\"))\n",
    "    end\n",
    "    # print data\n",
    "    for i in eachindex(lmm.data)\n",
    "        obs = lmm.data[i]\n",
    "        for j in 1:length(obs.y)\n",
    "            # id\n",
    "            print(io, i, \",\")\n",
    "            # Y\n",
    "            print(io, obs.y[j], \",\")\n",
    "            # X data\n",
    "            for k in 2:p\n",
    "                print(io, obs.X[j, k], \",\")\n",
    "            end\n",
    "            # Z data\n",
    "            for k in 2:q-1\n",
    "                print(io, obs.Z[j, k], \",\")\n",
    "            end\n",
    "            print(io, obs.Z[j, q], \"\\n\")\n",
    "        end\n",
    "    end\n",
    "end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.2.0",
   "language": "julia",
   "name": "julia-1.2"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
