{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Biostat 257 HW2\n",
    "### Caesar Z. Li\n",
    "### UID: 704135662\n",
    "\n",
    "## Q1. (Optional, 30 bonus pts) Derivatives\n",
    "Answer: show proof:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{eqnarray*}\n",
    "\\ell_i(\\boldsymbol{\\beta}, \\mathbf{L}, \\sigma^2) = - \\frac{n_i}{2} \\log (2\\pi) - \\frac{1}{2} \\log \\det \\boldsymbol{\\Omega}_i - \\frac{1}{2} (\\mathbf{y} - \\mathbf{X}_i \\boldsymbol{\\beta})^T \\boldsymbol{\\Omega}_i^{-1} (\\mathbf{y} - \\mathbf{X}_i \\boldsymbol{\\beta})\n",
    "\\end{eqnarray*}\n",
    "\n",
    "where\n",
    "\\begin{eqnarray*}\n",
    " \\boldsymbol{\\Omega}_i = \\sigma^2 \\mathbf{I}_{n_i} + \\mathbf{Z}_i \\boldsymbol{\\Sigma} \\mathbf{Z}_i^T = \\sigma^2 \\mathbf{I}_{n_i} + \\mathbf{Z}_i \\mathbf{L} \\mathbf{L}^T \\mathbf{Z}_i^T\n",
    "\\end{eqnarray*}\n",
    "\n",
    "Simpler way to find $\\nabla_{\\boldsymbol{\\beta}} \\ell_i$: use chain rule on $\\mathbf{r}_i$\n",
    "\\begin{eqnarray*}\n",
    "\\nabla_{\\boldsymbol{\\beta}} \\ell_i(\\boldsymbol{\\beta}, \\mathbf{L}, \\sigma^2) = \\mathbf{D}_{\\mathbf{r}_i} \\partial \\mathbf{r}^{T}_i \\mathbf{\\Omega}^{-1}\\mathbf{r}_i  \\cdot \\mathbf{D}_{\\boldsymbol{\\beta}} \\mathbf{r}_i \\\\\n",
    "= - (\\boldsymbol{\\Omega}^{-1}\\mathbf{r}_i)^T (-\\mathbf{X}_i) = \\mathbf{r}^T_i\\boldsymbol{\\Omega}^{-1}\\mathbf{X}_i = \\mathbf{X}_i\\boldsymbol{\\Omega}^{-1}\\mathbf{r}^T_i\n",
    "\\end{eqnarray*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, to find $\\nabla_{\\sigma^2} \\ell_i(\\boldsymbol{\\beta}, \\mathbf{L}, \\sigma^2)$, first find,\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "\\mathbf{D}_{\\boldsymbol{\\sigma}^2} \\boldsymbol{\\Omega} = vec\\mathbf{I}\n",
    "\\end{eqnarray*}\n",
    "\n",
    "And then,\n",
    "$$\n",
    "\\mathbf{D}_{\\boldsymbol{\\Omega}} \\ell_i = -\\frac{1}{2\\det(\\boldsymbol{\\Omega})} \\det(\\boldsymbol{\\Omega}) (\\boldsymbol{\\Omega}^T)^{-1} - \\mathbf{D}_{\\boldsymbol{\\Omega}}\\frac{1}{2}tr(\\mathbf{r}^T_i\\boldsymbol{\\Omega}^{-1}\\mathbf{r}_i)  \\\\\n",
    "= -\\frac{1}{2}(\\boldsymbol{\\Omega}^T)^{-1} - \\mathbf{D}_{\\boldsymbol{\\Omega}}\\frac{1}{2}tr(\\mathbf{r}_i\\mathbf{r}^T_i\\boldsymbol{\\Omega}^{-1}) \\\\\n",
    "= -\\frac{1}{2}\\boldsymbol{\\Omega}^{-1} + \\frac{1}{2}\\boldsymbol{\\Omega}^{-1}\\mathbf{r}_i\\mathbf{r}^T_i\\boldsymbol{\\Omega}^{-1}\n",
    "$$\n",
    "\n",
    "Use the chain rule,\n",
    "$$\n",
    "\\nabla_{\\sigma^2} \\ell_i(\\boldsymbol{\\beta}, \\mathbf{L}, \\sigma^2) = \\\\\n",
    "\\mathbf{D}_{\\boldsymbol{\\Omega}} \\ell_i \\cdot \\mathbf{D}_{\\boldsymbol{\\sigma}^2} \\boldsymbol{\\Omega} \\\\\n",
    "= -\\frac{1}{2}vec(\\boldsymbol{\\Omega}^{-1})^Tvec\\mathbf{I} + \\frac{1}{2}vec(\\boldsymbol{\\Omega}^{-1}\\mathbf{r}_i\\mathbf{r}^T_i\\boldsymbol{\\Omega}^{-1})^T vec\\mathbf{I} \\\\\n",
    "=-\\frac{1}{2}tr(\\boldsymbol{\\Omega}^T)^{-1} + \\frac{1}{2}tr(\\boldsymbol{\\Omega}^{-1}\\mathbf{r}_i\\mathbf{r}^T_i\\boldsymbol{\\Omega}^{-1}) \\\\\n",
    "=-\\frac{1}{2}tr(\\boldsymbol{\\Omega}^T)^{-1} + \\frac{1}{2}tr(\\boldsymbol{\\Omega}^{-1}\\boldsymbol{\\Omega}^{-1}\\mathbf{r}_i\\mathbf{r}^T_i)\\\\\n",
    "=-\\frac{1}{2}tr(\\boldsymbol{\\Omega}^T)^{-1} + \\frac{1}{2}tr(\\mathbf{r}^T_i\\boldsymbol{\\Omega}^{-1}\\boldsymbol{\\Omega}^{-1}\\mathbf{r}_i)\\\\\n",
    "= -\\frac{1}{2}tr(\\boldsymbol{\\Omega}^T)^{-1} + \\frac{1}{2}\\mathbf{r}^T_i\\boldsymbol{\\Omega}^{-2}\\mathbf{r}_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, to find $\\mathbf{D}_{\\boldsymbol{L}} \\ell_i$, first get,\n",
    "$$\n",
    "\\mathbf{D}_{\\mathbf{L}} \\boldsymbol{\\Omega} = \\mathbf{D}_{\\mathbf{L}}  (\\sigma^2 \\mathbf{I}_{n_i} + \\mathbf{Z}_i \\mathbf{L} \\mathbf{L}^T \\mathbf{Z}_i^T) \\\\\n",
    "= \\mathbf{Z}_i \\mathbf{L} \\otimes \\mathbf{Z}_i + (\\mathbf{Z}_i \\otimes \\mathbf{Z}_i \\mathbf{L}) \\mathbf{K}_{qq} \\\\\n",
    "= \\mathbf{Z}_i \\mathbf{L} \\otimes \\mathbf{Z}_i + \\mathbf{K}_{qq} (\\mathbf{Z}_i \\mathbf{L} \\otimes \\mathbf{Z}_i). \\\\\n",
    "$$\n",
    "Then,\n",
    "$$\n",
    "\\mathbf{D}_{\\boldsymbol{L}} \\ell_i = \\mathbf{D}_{\\boldsymbol{\\Omega}}\\ell_i \\cdot \\mathbf{D}_{\\mathbf{L}}\\boldsymbol{\\Omega} \\\\\n",
    "= -\\frac{1}{2}vec(\\boldsymbol{\\Omega}^{-1})^T(\\mathbf{Z}_i \\mathbf{L} \\otimes \\mathbf{Z}_i) - \\frac{1}{2}(\\mathbf{K}_{qq}vec(\\boldsymbol{\\Omega}^{-1}))^T(\\mathbf{Z}_i \\mathbf{L} \\otimes \\mathbf{Z}_i) + \\frac{1}{2}vec(\\boldsymbol{\\Omega}^{-1}\\mathbf{r}_i\\mathbf{r}^T_i\\boldsymbol{\\Omega}^{-1})^T(\\mathbf{Z}_i \\mathbf{L} \\otimes \\mathbf{Z}_i) + \\frac{1}{2}(\\mathbf{K}_{qq}vec(\\boldsymbol{\\Omega}^{-1}\\mathbf{r}_i\\mathbf{r}^T_i\\boldsymbol{\\Omega}^{-1}))^T(\\mathbf{Z}_i \\mathbf{L} \\otimes \\mathbf{Z}_i) \\\\\n",
    "= -\\frac{1}{2}((\\mathbf{L}^T\\mathbf{Z}^T_i  \\otimes \\mathbf{Z}^T_i)vec(\\boldsymbol{\\Omega}^{-1}))^T - -\\frac{1}{2}((\\mathbf{L}^T\\mathbf{Z}^T_i  \\otimes \\mathbf{Z}^T_i)vec(\\boldsymbol{\\Omega}^{-1}))^T  + \\frac{1}{2}((\\mathbf{L}^T\\mathbf{Z}^T_i  \\otimes \\mathbf{Z}^T_i)vec(\\boldsymbol{\\Omega}^{-1}\\mathbf{r}_i\\mathbf{r}^T_i\\boldsymbol{\\Omega}^{-1}))^T + \\frac{1}{2}((\\mathbf{L}^T\\mathbf{Z}^T_i  \\otimes \\mathbf{Z}^T_i)vec(\\boldsymbol{\\Omega}^{-1}\\mathbf{r}_i\\mathbf{r}^T_i\\boldsymbol{\\Omega}^{-1}))^T \\\\\n",
    "-vec(\\mathbf{Z}^T_i\\boldsymbol{\\Omega}^{-1}\\mathbf{Z}_i\\mathbf{L})^T + vec(\\mathbf{Z}^T_i\\boldsymbol{\\Omega}^{-1}\\mathbf{r}_i\\mathbf{r}^T_i\\boldsymbol{\\Omega}^{-1}\\mathbf{Z}_i\\mathbf{L})^T \\\\\n",
    "= -\\mathbf{Z}^T_i\\boldsymbol{\\Omega}^{-1}\\mathbf{Z}_i\\mathbf{L} + \\mathbf{Z}^T_i\\boldsymbol{\\Omega}^{-1}\\mathbf{r}_i\\mathbf{r}^T_i\\boldsymbol{\\Omega}^{-1}\\mathbf{Z}_i\\mathbf{L}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load necessary packages; make sure install them first\n",
    "using BenchmarkTools, CSV, DataFrames, DelimitedFiles, Distributions\n",
    "using Ipopt, LinearAlgebra, MathProgBase, MixedModels, NLopt\n",
    "using Random, RCall, Revise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2. (20 pts) Objective and gradient evaluator for a single datum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a type that holds an LMM datum\n",
    "struct LmmObs{T <: AbstractFloat}\n",
    "    # data\n",
    "    y          :: Vector{T}\n",
    "    X          :: Matrix{T}\n",
    "    Z          :: Matrix{T}\n",
    "    # arrays for holding gradient\n",
    "    ∇β         :: Vector{T}\n",
    "    ∇σ²        :: Vector{T}\n",
    "    ∇Σ         :: Matrix{T}    \n",
    "    # working arrays\n",
    "    # TODO: whatever intermediate arrays you may want to pre-allocate\n",
    "    yty        :: T\n",
    "    xty        :: Vector{T}\n",
    "    zty        :: Vector{T}\n",
    "    storage_p  :: Vector{T}\n",
    "    storage_q  :: Vector{T}\n",
    "    storage_q2 :: Vector{T}\n",
    "    storage_q3 :: Vector{T}\n",
    "    xtx        :: Matrix{T}\n",
    "    ztx        :: Matrix{T}\n",
    "    ztz        :: Matrix{T}\n",
    "    storage_qq :: Matrix{T}\n",
    "    storage_qq2 :: Matrix{T}\n",
    "    storage_qq3 :: Matrix{T}\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "logl!"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "    LmmObs(y::Vector, X::Matrix, Z::Matrix)\n",
    "\n",
    "Create an LMM datum of type `LmmObs`.\n",
    "\"\"\"\n",
    "function LmmObs(\n",
    "        y::Vector{T}, \n",
    "        X::Matrix{T}, \n",
    "        Z::Matrix{T}\n",
    "    ) where T <: AbstractFloat\n",
    "    n, p, q    = size(X, 1), size(X, 2), size(Z, 2)    \n",
    "    ∇β         = Vector{T}(undef, p)\n",
    "    ∇σ²        = Vector{T}(undef, 1)\n",
    "    ∇Σ         = Matrix{T}(undef, q, q)    \n",
    "    yty        = abs2(norm(y))\n",
    "    xty        = transpose(X) * y\n",
    "    zty        = transpose(Z) * y    \n",
    "    storage_p  = Vector{T}(undef, p)\n",
    "    storage_q  = Vector{T}(undef, q)\n",
    "    storage_q2 = Vector{T}(undef, q)\n",
    "    storage_q3 = Vector{T}(undef, q)\n",
    "    xtx        = transpose(X) * X\n",
    "    ztx        = transpose(Z) * X\n",
    "    ztz        = transpose(Z) * Z\n",
    "    storage_qq = similar(ztz)\n",
    "    storage_qq2 = similar(ztz)\n",
    "    storage_qq3 = similar(ztz)\n",
    "    LmmObs(y, X, Z, ∇β, ∇σ², ∇Σ, \n",
    "        yty, xty, zty, storage_p, storage_q, storage_q2, storage_q3, \n",
    "        xtx, ztx, ztz, storage_qq, storage_qq2, storage_qq3)\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "    logl!(obs::LmmObs, β, L, σ², needgrad=false)\n",
    "\n",
    "Evaluate the log-likelihood of a single LMM datum at parameter values `β`, `L`, \n",
    "and `σ²`. If `needgrad==true`, then `obs.∇β`, `obs.∇Σ`, and `obs.σ² are filled \n",
    "with the corresponding gradient.\n",
    "\"\"\"\n",
    "function logl!(\n",
    "        obs      :: LmmObs{T}, \n",
    "        β        :: Vector{T}, \n",
    "        L        :: Matrix{T}, \n",
    "        σ²       :: T,\n",
    "        needgrad :: Bool = true\n",
    "    ) where T <: AbstractFloat\n",
    "    n, p, q = size(obs.X, 1), size(obs.X, 2), size(obs.Z, 2)\n",
    "    ####################\n",
    "    # Evaluate objective\n",
    "    ####################    \n",
    "    # form the q-by-q matrix: M = σ² * I + Lt Zt Z L\n",
    "    copy!(obs.storage_qq, obs.ztz)\n",
    "    BLAS.trmm!('L', 'L', 'T', 'N', T(1), L, obs.storage_qq) # O(q^3)\n",
    "    BLAS.trmm!('R', 'L', 'N', 'N', T(1), L, obs.storage_qq) # O(q^3)\n",
    "    @inbounds for j in 1:q\n",
    "        obs.storage_qq[j, j] += σ²\n",
    "    end\n",
    "    # cholesky on M = σ² * I + Lt Zt Z L\n",
    "    LAPACK.potrf!('U', obs.storage_qq) # O(q^3)\n",
    "    # storage_q = (Mchol.U') \\ (Lt * (Zt * res))\n",
    "    BLAS.gemv!('N', T(-1), obs.ztx, β, T(1), copy!(obs.storage_q, obs.zty)) # O(pq)\n",
    "    BLAS.trmv!('L', 'T', 'N', L, obs.storage_q)    # O(q^2)\n",
    "    BLAS.trsv!('U', 'T', 'N', obs.storage_qq, obs.storage_q) # O(q^3)\n",
    "    # l2 norm of residual vector\n",
    "    copy!(obs.storage_p, obs.xty)\n",
    "    rtr  = obs.yty +\n",
    "        dot(β, BLAS.gemv!('N', T(1), obs.xtx, β, T(-2), obs.storage_p))\n",
    "    # assemble pieces\n",
    "    logl::T = n * log(2π) + (n - q) * log(σ²) # constant term\n",
    "    @inbounds for j in 1:q\n",
    "        logl += 2log(obs.storage_qq[j, j])\n",
    "    end\n",
    "    qf    = abs2(norm(obs.storage_q)) # quadratic form term\n",
    "    logl += (rtr - qf) / σ² \n",
    "    logl /= -2\n",
    "    ###################\n",
    "    # Evaluate gradient\n",
    "    ###################    \n",
    "    if needgrad\n",
    "        # TODO: fill ∇β, ∇L, ∇σ² by gradients\n",
    "        # form ∇β first:\n",
    "        # fill in ∇β with Xt(y - Xβ) = xty - XtXβ\n",
    "        copy!(obs.∇β, obs.xty)\n",
    "        BLAS.gemv!('N', T(-1), obs.xtx, β, T(1), obs.∇β)\n",
    "        # storage_q2 = (Mchol.U) \\ ( (Mchol.U') \\ (Lt * (Zt * res)) )\n",
    "        copy!(obs.storage_q2, obs.storage_q)\n",
    "        BLAS.trsv!('U', 'N', 'N', obs.storage_qq, obs.storage_q2)\n",
    "        BLAS.trmv!('L', 'N', 'N', L, obs.storage_q2) \n",
    "        # Put two pieces together, while makeing storage_q2 .= XtZ * storage_q2\n",
    "        BLAS.gemv!('T', T(-1), obs.ztx, obs.storage_q2, T(1), obs.∇β)\n",
    "        obs.∇β ./= σ²\n",
    "        \n",
    "        # form ∇σ²:\n",
    "        # expand rtΩ⁻²r into rtr + ( rt * ZLM⁻¹ Lt ) * ZtZ * ( LM⁻¹LtZt * r ) +\n",
    "        #  rt * ZLM⁻¹ LtZt * r \n",
    "        # Note the third term is already computed as 'qf' in earlier code\n",
    "        # rtr is computed as well, so I need only compute the second term.\n",
    "        BLAS.gemv!('N', T(1), obs.ztz, obs.storage_q2, T(0), obs.storage_q)\n",
    "        obs.∇σ²[1] = dot(obs.storage_q2, obs.storage_q)\n",
    "        obs.∇σ²[1] += rtr - 2qf\n",
    "        obs.∇σ²[1] /= σ²\n",
    "        # minus tr(Ω⁻¹)        \n",
    "        copy!(obs.storage_qq3, obs.ztz)\n",
    "        BLAS.trmm!('L', 'L', 'T', 'N', T(1), L, obs.storage_qq3)\n",
    "        LAPACK.potrs!('U', obs.storage_qq, obs.storage_qq3)\n",
    "        BLAS.trmm!('L', 'L', 'N', 'N', T(1), L, obs.storage_qq3)\n",
    "        #BLAS.gemm!('N', 'N', T(1), L, obs.storage_qq3, T(0), obs.storage_qq2)\n",
    "        @inbounds for j in 1:q\n",
    "             obs.∇σ²[1] += obs.storage_qq3[j, j]\n",
    "        end\n",
    "        obs.∇σ²[1] -= n \n",
    "        obs.∇σ²[1] /= (2 * σ²)\n",
    "        \n",
    "        # form ∇Σ:\n",
    "        # first part, get (-ZtZ + ZtZ * LM⁻¹Lt * ZtZ) \n",
    "        # or ZtZ (-I + LM⁻¹Lt * ZtZ)\n",
    "        # put first half in obs.storage_qq2\n",
    "        @inbounds for j in 1:q\n",
    "             obs.storage_qq3[j, j] -= 1.0\n",
    "        end\n",
    "        #BLAS.trmm!('L', 'L', 'N', 'N', T(1), L, obs.storage_qq3)\n",
    "        BLAS.gemm!('N', 'N', T(1), obs.ztz, obs.storage_qq3, T(0), obs.∇Σ)\n",
    "        \n",
    "        # second part, use Ω⁻¹r from earlier to get ZΩ⁻¹r \n",
    "        copy!(obs.storage_q, obs.zty)\n",
    "        BLAS.gemv!('N', T(- 1), obs.ztx, β, T(1), obs.storage_q)\n",
    "        # we have storage_q2 = L(Mchol.U) \\ ( (Mchol.U') \\ (Lt * (Zt * res)) )\n",
    "        # Put two pieces together, only this time storage_q2 .= ZtZ * storage_q2\n",
    "        BLAS.gemv!('N', T(- 1), obs.ztz, obs.storage_q2, T(1), obs.storage_q)\n",
    "        # outer product this vector, then don't * L to get dSigma\n",
    "        BLAS.gemm!('N', 'T', T(1 / σ²), obs.storage_q, obs.storage_q, T(1), obs.∇Σ)\n",
    "        obs.∇Σ ./= σ²\n",
    "        \n",
    "        #sleep(1e-3) # pretend this step takes 1ms\n",
    "    end    \n",
    "    ###################\n",
    "    # Return\n",
    "    ###################        \n",
    "    return logl    \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a good idea to test correctness and efficiency of the single datum objective/gradient evaluator here. First generate the same data set as in HW2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Random.seed!(257)\n",
    "# dimension\n",
    "n, p, q = 2000, 5, 3\n",
    "# predictors\n",
    "X  = [ones(n) randn(n, p - 1)]\n",
    "Z  = [ones(n) randn(n, q - 1)]\n",
    "# parameter values\n",
    "β  = [2.0; -1.0; rand(p - 2)]\n",
    "σ² = 1.5\n",
    "Σ  = fill(0.1, q, q) + 0.9I # compound symmetry \n",
    "L  = Matrix(cholesky(Symmetric(Σ)).L)\n",
    "# generate y\n",
    "y  = X * β + Z * rand(MvNormal(Σ)) + sqrt(σ²) * randn(n)\n",
    "\n",
    "# form the LmmObs object\n",
    "obs = LmmObs(y, X, Z);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1  Correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logl = logl!(obs, β, L, σ², true) = -3247.4568580638256\n",
      "obs.∇β = [-1.6309843232714532, -77.527515580418, -14.702372116010645, 6.978485518990293, -57.711823176822044]\n",
      "obs.∇σ² = [-4.820377758258701]\n",
      "obs.∇Σ = [1.6423791649320858 1.825024077223936 0.06127650043327201; 1.8250240772239479 0.11072391370464318 0.07213050869968186; 0.0612765004332986 0.07213050869966357 -1.0173748515301833]\n"
     ]
    }
   ],
   "source": [
    "@show logl = logl!(obs, β, L, σ², true)\n",
    "@show obs.∇β\n",
    "@show obs.∇σ²\n",
    "@show obs.∇Σ;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will lose all 20 points if following statement throws `AssertionError`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@assert abs(logl - (-3247.4568580638247)) < 1e-8\n",
    "@assert norm(obs.∇β - [-1.63098432327115, -77.52751558041871, -14.70237211601065, \n",
    "        6.978485518989568, -57.71182317682199]) < 1e-8\n",
    "@assert norm(obs.∇Σ - [1.6423791649290531 1.82502407722348 0.06127650043330721; \n",
    "        1.82502407722348 0.1107239137055005 0.07213050869971993; \n",
    "        0.06127650043330721 0.07213050869971993 -1.0173748515299939]) < 1e-8\n",
    "@assert abs(obs.∇σ²[1] - (-4.8203777582588145)) < 1e-8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2  Efficiency\n",
    "Benchmark for evaluating objective only. This is what we did in HW2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: \n",
       "  memory estimate:  0 bytes\n",
       "  allocs estimate:  0\n",
       "  --------------\n",
       "  minimum time:     1.827 μs (0.00% GC)\n",
       "  median time:      1.837 μs (0.00% GC)\n",
       "  mean time:        1.850 μs (0.00% GC)\n",
       "  maximum time:     43.715 μs (0.00% GC)\n",
       "  --------------\n",
       "  samples:          10000\n",
       "  evals/sample:     10"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@benchmark logl!($obs, $β, $L, $σ², false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: \n",
       "  memory estimate:  0 bytes\n",
       "  allocs estimate:  0\n",
       "  --------------\n",
       "  minimum time:     7.126 μs (0.00% GC)\n",
       "  median time:      7.278 μs (0.00% GC)\n",
       "  mean time:        7.337 μs (0.00% GC)\n",
       "  maximum time:     23.092 μs (0.00% GC)\n",
       "  --------------\n",
       "  samples:          10000\n",
       "  evals/sample:     5"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm_objgrad = @benchmark logl!($obs, $β, $L, $σ², true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My median runt time is 2.1μs. You will get full credit (10 pts) if the median run time is within 10μs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  The points you will get are\n",
    "clamp(10 / (median(bm_objgrad).time / 1e3) * 10, 0, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3. LmmModel type\n",
    "We create a `LmmModel` type to hold all data points and model parameters. Log-likelihood/gradient of a LmmModel object is simply the sum of log-likelihood/gradient of individual data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a type that holds LMM model (data + parameters)\n",
    "struct LmmModel{T <: AbstractFloat} <: MathProgBase.AbstractNLPEvaluator\n",
    "    # data\n",
    "    data :: Vector{LmmObs{T}}\n",
    "    # parameters\n",
    "    β    :: Vector{T}\n",
    "    L    :: Matrix{T}\n",
    "    σ²   :: Vector{T}    \n",
    "    # arrays for holding gradient\n",
    "    ∇β   :: Vector{T}\n",
    "    ∇σ²  :: Vector{T}\n",
    "    ∇L   :: Matrix{T}\n",
    "    # TODO: add whatever intermediate arrays you may want to pre-allocate\n",
    "    xty  :: Vector{T}\n",
    "    ztr2 :: Vector{T}\n",
    "    xtx  :: Matrix{T}\n",
    "    ztz2 :: Matrix{T}\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "logl!"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "    LmmModel(data::Vector{LmmObs})\n",
    "\n",
    "Create an LMM model that contains data and parameters.\n",
    "\"\"\"\n",
    "function LmmModel(obsvec::Vector{LmmObs{T}}) where T <: AbstractFloat\n",
    "    # dims\n",
    "    p    = size(obsvec[1].X, 2)\n",
    "    q    = size(obsvec[1].Z, 2)\n",
    "    # parameters\n",
    "    β    = Vector{T}(undef, p)\n",
    "    L    = Matrix{T}(undef, q, q)\n",
    "    σ²   = Vector{T}(undef, 1)    \n",
    "    # gradients\n",
    "    ∇β   = similar(β)    \n",
    "    ∇σ²  = similar(σ²)\n",
    "    ∇L   = similar(L)\n",
    "    # intermediate arrays\n",
    "    xty  = Vector{T}(undef, p)\n",
    "    ztr2 = Vector{T}(undef, abs2(q))\n",
    "    xtx  = Matrix{T}(undef, p, p)\n",
    "    ztz2 = Matrix{T}(undef, abs2(q), abs2(q))\n",
    "    LmmModel(obsvec, β, L, σ², ∇β, ∇σ², ∇L, xty, ztr2, xtx, ztz2)\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "    logl!(m::LmmModel, needgrad=false)\n",
    "\n",
    "Evaluate the log-likelihood of an LMM model at parameter values `m.β`, `m.L`, \n",
    "and `m.σ²`. If `needgrad==true`, then `m.∇β`, `m.∇Σ`, and `m.σ² are filled \n",
    "with the corresponding gradient.\n",
    "\"\"\"\n",
    "function logl!(m::LmmModel{T}, needgrad::Bool = false) where T <: AbstractFloat\n",
    "    logl = zero(T)\n",
    "    if needgrad\n",
    "        fill!(m.∇β , 0)\n",
    "        fill!(m.∇L , 0)\n",
    "        fill!(m.∇σ², 0)        \n",
    "    end\n",
    "    @inbounds for i in 1:length(m.data)\n",
    "        obs = m.data[i]\n",
    "        logl += logl!(obs, m.β, m.L, m.σ²[1], needgrad)\n",
    "        if needgrad\n",
    "            BLAS.axpy!(T(1), obs.∇β, m.∇β)\n",
    "            BLAS.axpy!(T(1), obs.∇Σ, m.∇L)\n",
    "            m.∇σ²[1] += obs.∇σ²[1]\n",
    "        end\n",
    "    end\n",
    "    # obtain gradient wrt L: m.∇L = m.∇L * L\n",
    "    if needgrad\n",
    "        # TODO \n",
    "        #m.∇L .= m.∇Σ * m.L\n",
    "        BLAS.trmm!('R', 'L', 'N', 'N', T(1), m.L, m.∇L)\n",
    "    end\n",
    "    logl\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4. (20 pts) Test data\n",
    "Let's generate a fake longitudinal data set to test our algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Random.seed!(257)\n",
    "\n",
    "# dimension\n",
    "m      = 1000 # number of individuals\n",
    "ns     = rand(1500:2000, m) # numbers of observations per individual\n",
    "p      = 5 # number of fixed effects, including intercept\n",
    "q      = 3 # number of random effects, including intercept\n",
    "obsvec = Vector{LmmObs{Float64}}(undef, m)\n",
    "# true parameter values\n",
    "βtrue  = [0.1; 6.5; -3.5; 1.0; 5]\n",
    "σ²true = 1.5\n",
    "σtrue  = sqrt(σ²true)\n",
    "Σtrue  = Matrix(Diagonal([2.0; 1.2; 1.0]))\n",
    "Ltrue  = Matrix(cholesky(Symmetric(Σtrue)).L)\n",
    "# generate data\n",
    "for i in 1:m\n",
    "    # first column intercept, remaining entries iid std normal\n",
    "    X = Matrix{Float64}(undef, ns[i], p)\n",
    "    X[:, 1] .= 1\n",
    "    @views Distributions.rand!(Normal(), X[:, 2:p])\n",
    "    # first column intercept, remaining entries iid std normal\n",
    "    Z = Matrix{Float64}(undef, ns[i], q)\n",
    "    Z[:, 1] .= 1\n",
    "    @views Distributions.rand!(Normal(), Z[:, 2:q])\n",
    "    # generate y\n",
    "    y = X * βtrue .+ Z * (Ltrue * randn(q)) .+ σtrue * randn(ns[i])\n",
    "    # form a LmmObs instance\n",
    "    obsvec[i] = LmmObs(y, X, Z)\n",
    "end\n",
    "# form a LmmModel instance\n",
    "lmm = LmmModel(obsvec);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For later comparison with other software, we save the data into a text file lmm_data.txt. Do not put this file in Git. It takes 246.6MB storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "(isfile(\"lmm_data.txt\") && filesize(\"lmm_data.txt\") == 246638945) || \n",
    "open(\"lmm_data.txt\", \"w\") do io\n",
    "    p = size(lmm.data[1].X, 2)\n",
    "    q = size(lmm.data[1].Z, 2)\n",
    "    # print header\n",
    "    print(io, \"ID,Y,\")\n",
    "    for j in 1:(p-1)\n",
    "        print(io, \"X\" * string(j) * \",\")\n",
    "    end\n",
    "    for j in 1:(q-1)\n",
    "        print(io, \"Z\" * string(j) * (j < q-1 ? \",\" : \"\\n\"))\n",
    "    end\n",
    "    # print data\n",
    "    for i in eachindex(lmm.data)\n",
    "        obs = lmm.data[i]\n",
    "        for j in 1:length(obs.y)\n",
    "            # id\n",
    "            print(io, i, \",\")\n",
    "            # Y\n",
    "            print(io, obs.y[j], \",\")\n",
    "            # X data\n",
    "            for k in 2:p\n",
    "                print(io, obs.X[j, k], \",\")\n",
    "            end\n",
    "            # Z data\n",
    "            for k in 2:q-1\n",
    "                print(io, obs.Z[j, k], \",\")\n",
    "            end\n",
    "            print(io, obs.Z[j, q], \"\\n\")\n",
    "        end\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1  Correctness\n",
    "Evaluate log-likelihood and gradient of whole data set at the true parameter values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obj = logl!(lmm, true) = -2.8547126486833044e6\n",
      "lmm.∇β = [-8.69374436063768, 169.38364540289376, 1412.9462826020176, 583.9807190830627, 57.76586042026756]\n",
      "lmm.∇σ² = [-371.82886426343083]\n",
      "lmm.∇L = [20.19709774965016 -4.566719695066214 9.073934365208512; -5.895609775262915 -13.456093353754492 -18.889943728354076; 12.832481043361337 -20.692896580045577 -61.98953657916843]\n"
     ]
    }
   ],
   "source": [
    "copy!(lmm.β, βtrue)\n",
    "copy!(lmm.L, Ltrue)\n",
    "lmm.σ²[1] = σ²true\n",
    "@show obj = logl!(lmm, true)\n",
    "@show lmm.∇β\n",
    "@show lmm.∇σ²\n",
    "@show lmm.∇L;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test correctness. You will loss all 20 points if following code throws AssertError."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "@assert abs(obj - (-2.854712648683302e6)) < 1e-6\n",
    "@assert norm(lmm.∇β - [-8.693744360651923, 169.38364540290684, \n",
    "        1412.9462826018173, 583.9807190830952, 57.76586042024306]) < 1e-6\n",
    "@assert norm(lmm.∇L - [20.197097749713322 -4.566719695067792 9.073934365205824; \n",
    "        -5.895609775264991 -13.456093353707153 -18.889943728349024; \n",
    "        12.832481043357378 -20.69289658004 -61.98953657919662]) < 1e-6\n",
    "@assert abs(lmm.∇σ²[1] - (-371.8288642639822)) < 1e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2  Efficiency\n",
    "Test efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: \n",
       "  memory estimate:  0 bytes\n",
       "  allocs estimate:  0\n",
       "  --------------\n",
       "  minimum time:     2.177 ms (0.00% GC)\n",
       "  median time:      2.339 ms (0.00% GC)\n",
       "  mean time:        2.535 ms (0.00% GC)\n",
       "  maximum time:     73.461 ms (0.00% GC)\n",
       "  --------------\n",
       "  samples:          1969\n",
       "  evals/sample:     1"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm_model = @benchmark logl!($lmm, true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My median run time is 2.122ms. You will get full credit if your median run time is within 10ms. The points you will get are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.0"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clamp(10 / (median(bm_model).time / 1e6) * 10, 0, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3  Memory\n",
    "You will lose 1 point for each 100 bytes memory allocation. So the points you will get is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.0"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clamp(10 - median(bm_model).memory / 100, 0, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5  Q5. (30 pts) Starting point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$$\n",
    "    \\text{minimize} \\sum_i \\| \\mathbf{r}_i^{(0)} \\mathbf{r}_i^{(0)T} - \\mathbf{Z}_i \\boldsymbol{\\Sigma} \\mathbf{Z}_i^T \\|_{\\text{F}}^2.\n",
    "$$\n",
    "Let $ \\mathbf{r}_i^{(0)} \\mathbf{r}_i^{(0)T} - \\mathbf{Z}_i \\boldsymbol{\\Sigma} \\mathbf{Z}_i^T = \\mathbf{A}_i$. Then we are minimizing $\\sum_i tr(\\mathbf{A}^T_i\\mathbf{A}_i)$. We can minimize by setting its gradient to zero.\n",
    "$$\n",
    "\\mathbf{D}_{\\boldsymbol{\\Sigma}} \\mathbf{A}_i = \\mathbf{Z}_i \\otimes \\mathbf{Z}_i \\\\\n",
    "\\mathbf{D}_{\\mathbf{A}_i} \\text{objective} = 2 \\sum_i \\mathbf{A}_i \\\\\n",
    "\\mathbf{D}_{\\boldsymbol{\\Sigma}} \\text{objective} = \\mathbf{D}_{\\mathbf{A}_i} \\text{objective} \\cdot \\mathbf{D}_{\\boldsymbol{\\Sigma}} \\mathbf{A}_i \\\\\n",
    "= 2 (vec(\\mathbf{A}_i))^T (\\mathbf{Z}_i \\otimes \\mathbf{Z}_i) \\\\\n",
    "= 2 (\\mathbf{Z}_i \\otimes \\mathbf{Z}_i)vec(\\mathbf{A}_i) \\\\\n",
    "= 2 vec\\mathbf{Z}^T_i\\mathbf{A}_i\\mathbf{Z}_i = 2 \\mathbf{Z}^T_i\\mathbf{A}_i\\mathbf{Z}_i \\\\\n",
    "\\text{set to}= 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\implies \\sum_i (\\mathbf{Z}^T_i\\mathbf{r}_i\\mathbf{r}^T_i\\mathbf{Z}_i - \\mathbf{Z}^T_i\\mathbf{Z}_i \\boldsymbol{\\Sigma^{(0)}} \\mathbf{Z}^T_i\\mathbf{Z}_i) = 0 \\\\\n",
    "\\implies \\sum_i \\mathbf{Z}^T_i\\mathbf{r}_i\\mathbf{r}^T_i\\mathbf{Z}_i = \\sum_i \\mathbf{Z}^T_i\\mathbf{Z}_i \\boldsymbol{\\Sigma^{(0)}} \\mathbf{Z}^T_i\\mathbf{Z}_i \\\\\n",
    "\\implies \\sum_i vec(\\mathbf{Z}^T_i\\mathbf{r}_i\\mathbf{r}^T_i\\mathbf{Z}_i) = \\sum_i vec(\\mathbf{Z}^T_i\\mathbf{Z}_i \\boldsymbol{\\Sigma^{(0)}} \\mathbf{Z}^T_i\\mathbf{Z}_i) \\\\\n",
    "\\implies (\\sum_i\\mathbf{Z}^T_i\\mathbf{r}_i \\otimes \\mathbf{r}^T_i\\mathbf{Z}_i) = (\\sum_i\\mathbf{Z}^T_i\\mathbf{Z}_i \\otimes \\mathbf{Z}^T_i\\mathbf{Z}_i)vec(\\boldsymbol{\\Sigma^{(0)}}) \\\\\n",
    "\\implies vec(\\boldsymbol{\\Sigma^{(0)}}) = (\\sum_i\\mathbf{Z}^T_i\\mathbf{Z}_i \\otimes \\mathbf{Z}^T_i\\mathbf{Z}_i)^{-1} (\\sum_i \\mathbf{Z}^T_i\\mathbf{r}_i \\otimes \\mathbf{r}^T_i\\mathbf{Z}_i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implement this start point strategy in the function `init_ls()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "init_ls!"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    init_ls!(m::LmmModel)\n",
    "\n",
    "Initialize parameters of a `LmmModel` object from the least squares estimate. \n",
    "`m.β`, `m.L`, and `m.σ²` are overwritten with the least squares estimates.\n",
    "\"\"\"\n",
    "function init_ls!(m::LmmModel{T}) where T <: AbstractFloat\n",
    "    p, q = size(m.data[1].X, 2), size(m.data[1].Z, 2)\n",
    "    # TODO: fill m.β, m.L, m.σ² by LS estimates\n",
    "    # get inv(sum(xtx)) and then m.β, and get m.L,\n",
    "    fill!(m.β, 0)\n",
    "    fill!(m.xtx, 0)\n",
    "    fill!(m.xty, 0)\n",
    "    fill!(m.L, 0)\n",
    "    fill!(m.ztr2, 0)\n",
    "    fill!(m.ztz2, 0)  \n",
    "    m.σ²[1] = 0.0\n",
    "    ns = 0\n",
    "    @inbounds for i in 1:length(m.data)\n",
    "        obs = m.data[i]\n",
    "        BLAS.axpy!(T(1), obs.xtx, m.xtx)\n",
    "        BLAS.axpy!(T(1), obs.xty, m.β)\n",
    "        #BLAS.gemv!('N', T(2), obs.ztx, m.L, T(1), m.L)\n",
    "    end\n",
    "    LAPACK.potrf!('U', m.xtx)\n",
    "    LAPACK.potrs!('U', m.xtx, m.β)\n",
    "    \n",
    "    # Get m.σ² and Σ\n",
    "    @inbounds for i in 1:length(m.data)\n",
    "        obs = m.data[i]\n",
    "        m.σ²[1] += obs.yty\n",
    "        BLAS.gemv!('N', T(1), obs.xtx, m.β, T(0), m.xty)\n",
    "        m.σ²[1] += dot(m.β, m.xty)\n",
    "        m.σ²[1] -= 2dot(m.β, obs.xty)\n",
    "        ns += size(obs.X, 1)\n",
    "        # get Σ\n",
    "        # get ztxβ, then zty - ztxβ\n",
    "        BLAS.gemv!('N', T(1), obs.ztx, m.β, T(0), obs.storage_q3)\n",
    "        BLAS.axpby!(T(1), obs.zty, T(-1), obs.storage_q3)\n",
    "        # get Kronecker product from lefthand side\n",
    "        BLAS.gemm!('N', 'T', T(1), obs.storage_q3, obs.storage_q3, T(0), obs.storage_qq2)\n",
    "        m.ztr2 .+= Base.vec(obs.storage_qq2)\n",
    "        m.ztz2 .+= Base.kron(obs.ztz, obs.ztz)\n",
    "    end\n",
    "    m.σ²[1] = m.σ²[1] / ns\n",
    "    LAPACK.potrf!('U', m.ztz2)\n",
    "    LAPACK.potrs!('U', m.ztz2, m.ztr2)\n",
    "    m.L .= reshape(m.ztr2, q, q)\n",
    "    LAPACK.potrf!('U', m.L)\n",
    "    \n",
    "    #sleep(1e-3) # pretend this takes 1ms\n",
    "    m\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logl!(lmm) = -3.375070980624222e6\n",
      "lmm.β = [0.08757868042872421, 6.49874064773743, -3.4970528646610326, 0.9971817478195717, 5.000992653697893]\n",
      "lmm.σ² = [5.66724466129435]\n",
      "lmm.L = [1.4307910693942878 -0.0020939271680232513 0.013217721346714471; -0.002995972291969741 1.08951604771318 -0.011539720315817311; 0.018911797660621295 -0.012600387416032015 0.9605029322102014]\n"
     ]
    }
   ],
   "source": [
    "init_ls!(lmm)\n",
    "@show logl!(lmm)\n",
    "@show lmm.β\n",
    "@show lmm.σ²\n",
    "@show lmm.L;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1  Correctness\n",
    "Your start points should have a log-likelihood larger than -3.375071 (10 pts). The points you get are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is the points you get\n",
    "(logl!(lmm) >  -3.375071e6) * 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2  Efficiency\n",
    "The start point should be computed quickly. Otherwise there is no point using it as a starting point. You get full credit (10 pts) if the median run time is within 1ms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: \n",
       "  memory estimate:  796.97 KiB\n",
       "  allocs estimate:  3002\n",
       "  --------------\n",
       "  minimum time:     589.320 μs (0.00% GC)\n",
       "  median time:      676.489 μs (0.00% GC)\n",
       "  mean time:        804.261 μs (10.70% GC)\n",
       "  maximum time:     13.486 ms (92.86% GC)\n",
       "  --------------\n",
       "  samples:          6194\n",
       "  evals/sample:     1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm_init = @benchmark init_ls!($lmm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is the points you get\n",
    "clamp(1 / (median(bm_init).time / 1e6) * 10, 0, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6  Q6. NLP via MathProgBase.jl\n",
    "We define the NLP problem using the modelling tool MathProgBase.jl. Start-up code is given below. Modify if necessary to accomodate your own code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    fit!(m::LmmModel, solver=Ipopt.IpoptSolver(print_level=5))\n",
    "\n",
    "Fit an `LmmModel` object by MLE using a nonlinear programming solver. Start point \n",
    "should be provided in `m.β`, `m.σ²`, `m.L`.\n",
    "\"\"\"\n",
    "function fit!(\n",
    "        m::LmmModel,\n",
    "        solver=Ipopt.IpoptSolver(print_level=5)\n",
    "    )\n",
    "    p    = size(m.data[1].X, 2)\n",
    "    q    = size(m.data[1].Z, 2)\n",
    "    npar = p + ((q * (q + 1)) >> 1) + 1\n",
    "    optm = MathProgBase.NonlinearModel(solver)\n",
    "    # set lower bounds and upper bounds of parameters\n",
    "    # diagonal entries of Cholesky factor L should be >= 0\n",
    "    lb   = fill(-Inf, npar)\n",
    "    ub   = fill( Inf, npar)\n",
    "    offset = p + 1\n",
    "    for j in 1:q, i in j:q\n",
    "        i == j && (lb[offset] = 0)\n",
    "        offset += 1\n",
    "    end\n",
    "    # σ² should be >= 0\n",
    "    lb[end] = 0\n",
    "    MathProgBase.loadproblem!(optm, npar, 0, lb, ub, Float64[], Float64[], :Max, m)\n",
    "    # starting point\n",
    "    par0 = zeros(npar)\n",
    "    modelpar_to_optimpar!(par0, m)\n",
    "    MathProgBase.setwarmstart!(optm, par0)\n",
    "    # optimize\n",
    "    MathProgBase.optimize!(optm)\n",
    "    optstat = MathProgBase.status(optm)\n",
    "    optstat == :Optimal || @warn(\"Optimization unsuccesful; got $optstat\")\n",
    "    # update parameters and refresh gradient\n",
    "    optimpar_to_modelpar!(m, MathProgBase.getsolution(optm))\n",
    "    logl!(m, true)\n",
    "    m\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "    modelpar_to_optimpar!(par, m)\n",
    "\n",
    "Translate model parameters in `m` to optimization variables in `par`.\n",
    "\"\"\"\n",
    "function modelpar_to_optimpar!(\n",
    "        par :: Vector,\n",
    "        m   :: LmmModel\n",
    "    )\n",
    "    p = size(m.data[1].X, 2)\n",
    "    q = size(m.data[1].Z, 2)\n",
    "    # β\n",
    "    copyto!(par, m.β)\n",
    "    # L\n",
    "    offset = p + 1\n",
    "    @inbounds for j in 1:q, i in j:q\n",
    "        par[offset] = m.L[i, j]\n",
    "        offset += 1\n",
    "    end\n",
    "    # σ²\n",
    "    par[end] = m.σ²[1]\n",
    "    par\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "    optimpar_to_modelpar!(m, par)\n",
    "\n",
    "Translate optimization variables in `par` to the model parameters in `m`.\n",
    "\"\"\"\n",
    "function optimpar_to_modelpar!(\n",
    "        m   :: LmmModel, \n",
    "        par :: Vector\n",
    "    )\n",
    "    p = size(m.data[1].X, 2)\n",
    "    q = size(m.data[1].Z, 2)\n",
    "    # β\n",
    "    copyto!(m.β, 1, par, 1, p)\n",
    "    # L\n",
    "    fill!(m.L, 0)\n",
    "    offset = p + 1\n",
    "    @inbounds for j in 1:q, i in j:q\n",
    "        m.L[i, j] = par[offset]\n",
    "        offset   += 1\n",
    "    end\n",
    "    # σ²\n",
    "    m.σ²[1] = par[end]    \n",
    "    m\n",
    "end\n",
    "\n",
    "function MathProgBase.initialize(\n",
    "        m                  :: LmmModel, \n",
    "        requested_features :: Vector{Symbol}\n",
    "    )\n",
    "    for feat in requested_features\n",
    "        if !(feat in [:Grad])\n",
    "            error(\"Unsupported feature $feat\")\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "MathProgBase.features_available(m::LmmModel) = [:Grad]\n",
    "\n",
    "function MathProgBase.eval_f(\n",
    "        m   :: LmmModel, \n",
    "        par :: Vector\n",
    "    )\n",
    "    optimpar_to_modelpar!(m, par)\n",
    "    logl!(m, false) # don't need gradient here\n",
    "end\n",
    "\n",
    "function MathProgBase.eval_grad_f(\n",
    "        m    :: LmmModel, \n",
    "        grad :: Vector, \n",
    "        par  :: Vector\n",
    "    )\n",
    "    p = size(m.data[1].X, 2)\n",
    "    q = size(m.data[1].Z, 2)\n",
    "    optimpar_to_modelpar!(m, par) \n",
    "    obj = logl!(m, true)\n",
    "    # gradient wrt β\n",
    "    copyto!(grad, m.∇β)\n",
    "    # gradient wrt L\n",
    "    offset = p + 1\n",
    "    @inbounds for j in 1:q, i in j:q\n",
    "        grad[offset] = m.∇L[i, j]\n",
    "        offset += 1\n",
    "    end\n",
    "    # gradient with respect to σ²\n",
    "    grad[end] = m.∇σ²[1]\n",
    "    # return objective\n",
    "    obj\n",
    "end\n",
    "\n",
    "MathProgBase.eval_g(m::LmmModel, g, par) = nothing\n",
    "MathProgBase.jac_structure(m::LmmModel) = Int[], Int[]\n",
    "MathProgBase.eval_jac_g(m::LmmModel, J, par) = nothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7  Q7. (20 pts) Test drive\n",
    "Now we can run our NLP solver to compute the MLE. For grading purpose, we first use the :LD_LBFGS (limited-memory BFGS) algorithm in NLopt.jl here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective value at starting point: -3.375070980624222e6\n",
      "\n",
      "  0.777064 seconds (1.58 M allocations: 73.614 MiB)\n",
      "objective value at solution: -2.8547097852113894e6\n",
      "\n",
      "solution values:\n",
      "lmm.β = [0.08237464781054629, 6.500144678053184, -3.498791279668385, 1.0004995438209268, 5.000049494751214]\n",
      "lmm.σ² = [1.4990428254359849]\n",
      "lmm.L * transpose(lmm.L) = [2.0569561620228005 -0.010074595940203004 0.018527099248931592; -0.010074595940203004 1.1822481218686605 -0.02268192815382304; 0.018527099248931592 -0.02268192815382304 0.9379055458283312]\n",
      "gradient @ solution:\n",
      "lmm.∇β = [-0.025276005391447143, -0.004816389129828735, 0.016564279758565448, -0.0008153107416575267, 0.03471473220162835]\n",
      "lmm.∇σ² = [-0.059482263596102314]\n",
      "lmm.∇L = [-0.02816976625617268 0.0746616094448793 -0.024956777417154626; 0.09733211273999222 0.030356434621579678 -0.01885979732603346; -0.0368181557016089 -0.02120634330968573 0.001192501009617314]\n",
      "sqrt(abs2(norm(lmm.∇β)) + abs2(norm(lmm.∇σ²) + abs2(norm(LowerTriangular(lmm.∇L))))) = 0.08599548547514116\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.08599548547514116"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize from least squares\n",
    "init_ls!(lmm)\n",
    "println(\"objective value at starting point: \", logl!(lmm)); println()\n",
    "\n",
    "@time fit!(lmm, NLopt.NLoptSolver(algorithm=:LD_LBFGS, \n",
    "        ftol_rel = 1e-12, ftol_abs = 1e-12, \n",
    "        xtol_rel = 1e-12, xtol_abs = 1e-12, \n",
    "        maxeval=10000));\n",
    "\n",
    "println(\"objective value at solution: \", logl!(lmm)); println()\n",
    "println(\"solution values:\")\n",
    "@show lmm.β\n",
    "@show lmm.σ²\n",
    "@show lmm.L * transpose(lmm.L)\n",
    "println(\"gradient @ solution:\")\n",
    "@show lmm.∇β\n",
    "@show lmm.∇σ²\n",
    "@show lmm.∇L\n",
    "@show sqrt(abs2(norm(lmm.∇β)) + abs2(norm(lmm.∇σ²) + \n",
    "        abs2(norm(LowerTriangular(lmm.∇L)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1  Correctness\n",
    "You get 10 points if the following code does not throw AssertError."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# objective at solution should be close enough to the optimal\n",
    "@assert logl!(lmm) > -2.85471e6\n",
    "# gradient at solution should be small enough\n",
    "@assert sqrt(abs2(norm(lmm.∇β)) + abs2(norm(lmm.∇σ²) + \n",
    "        abs2(norm(LowerTriangular(lmm.∇L))))) < 0.15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2  Efficiency\n",
    "My median run time is 132.5ms. You get 10 points if your median time is within 1s(=1000ms)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: \n",
       "  memory estimate:  11.02 KiB\n",
       "  allocs estimate:  237\n",
       "  --------------\n",
       "  minimum time:     137.567 ms (0.00% GC)\n",
       "  median time:      144.505 ms (0.00% GC)\n",
       "  mean time:        146.997 ms (0.00% GC)\n",
       "  maximum time:     164.424 ms (0.00% GC)\n",
       "  --------------\n",
       "  samples:          34\n",
       "  evals/sample:     1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm_bfgs = @benchmark fit!($lmm, $(NLopt.NLoptSolver(algorithm=:LD_LBFGS, \n",
    "        ftol_rel = 1e-12, ftol_abs = 1e-12, \n",
    "        xtol_rel = 1e-12, xtol_abs = 1e-12, \n",
    "        maxeval = 10000))) setup = (init_ls!(lmm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is the points you get\n",
    "clamp(1 / (median(bm_bfgs).time / 1e9) * 10, 0, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8  Q8. (10 pts) Gradient free vs gradient-based methods\n",
    "Advantage of using a modelling tool such as MathProgBase.jl is that we can easily switch the backend solvers. For a research problem, we never know beforehand which solvers work better than the others.\n",
    "Try different solvers in the NLopt.jl and Ipopt.jl packages. Compare the results in terms run times (the shorter the better), objective values at solution (the larger the better), and gradients at solution (closer to 0 the better). Summarize what you find.\n",
    "See this page for the descriptions of algorithms in NLopt.\n",
    "Documentation for the Ipopt can be found here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "******************************************************************************\n",
      "This program contains Ipopt, a library for large-scale nonlinear optimization.\n",
      " Ipopt is released as open source code under the Eclipse Public License (EPL).\n",
      "         For more information visit http://projects.coin-or.org/Ipopt\n",
      "******************************************************************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# vector of solvers to compare\n",
    "solvers = [\n",
    "    # NLopt: gradient-based algorithms\n",
    "    NLopt.NLoptSolver(algorithm=:LD_LBFGS, \n",
    "        ftol_rel = 1e-12, ftol_abs = 1e-12,\n",
    "        xtol_rel = 1e-12, xtol_abs = 1e-12,\n",
    "        maxeval=10000),\n",
    "    NLopt.NLoptSolver(algorithm=:LD_MMA, \n",
    "        ftol_rel = 1e-12, ftol_abs = 1e-12, \n",
    "        xtol_rel = 1e-12, xtol_abs = 1e-12, \n",
    "        maxeval=10000),\n",
    "    # NLopt: gradient-free algorithms\n",
    "    NLopt.NLoptSolver(algorithm=:LN_BOBYQA, \n",
    "        ftol_rel = 1e-12, ftol_abs = 1e-12, \n",
    "        xtol_rel = 1e-12, xtol_abs = 1e-12, \n",
    "        maxeval=10000),\n",
    "    # Ipopt\n",
    "    Ipopt.IpoptSolver(print_level=0)\n",
    "]\n",
    "# containers for results\n",
    "runtime = zeros(length(solvers))\n",
    "objvals = zeros(length(solvers))\n",
    "gradnrm = zeros(length(solvers))\n",
    "\n",
    "for (i, solver) in enumerate(solvers)\n",
    "    bm = @benchmark fit!($lmm, $solver) setup = (init_ls!(lmm))\n",
    "    runtime[i] = median(bm).time / 1e9\n",
    "    objvals[i] = logl!(lmm, true)\n",
    "    gradnrm[i] = sqrt(abs2(norm(lmm.∇β)) + abs2(norm(lmm.∇σ²) + \n",
    "        abs2(norm(LowerTriangular(lmm.∇L)))))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>Runtime</th><th>Objective</th><th>Gradnorm</th></tr><tr><th></th><th>Float64</th><th>Float64</th><th>Float64</th></tr></thead><tbody><p>4 rows × 3 columns</p><tr><th>1</th><td>0.158832</td><td>-2.85471e6</td><td>0.0859955</td></tr><tr><th>2</th><td>0.161323</td><td>-2.85471e6</td><td>0.00449421</td></tr><tr><th>3</th><td>0.0604673</td><td>-2.85603e6</td><td>35203.7</td></tr><tr><th>4</th><td>5.37031</td><td>-2.85471e6</td><td>1.55889e-5</td></tr></tbody></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ccc}\n",
       "\t& Runtime & Objective & Gradnorm\\\\\n",
       "\t\\hline\n",
       "\t& Float64 & Float64 & Float64\\\\\n",
       "\t\\hline\n",
       "\t1 & 0.158832 & -2.85471e6 & 0.0859955 \\\\\n",
       "\t2 & 0.161323 & -2.85471e6 & 0.00449421 \\\\\n",
       "\t3 & 0.0604673 & -2.85603e6 & 35203.7 \\\\\n",
       "\t4 & 5.37031 & -2.85471e6 & 1.55889e-5 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "4×3 DataFrame\n",
       "│ Row │ Runtime   │ Objective  │ Gradnorm   │\n",
       "│     │ \u001b[90mFloat64\u001b[39m   │ \u001b[90mFloat64\u001b[39m    │ \u001b[90mFloat64\u001b[39m    │\n",
       "├─────┼───────────┼────────────┼────────────┤\n",
       "│ 1   │ 0.158832  │ -2.85471e6 │ 0.0859955  │\n",
       "│ 2   │ 0.161323  │ -2.85471e6 │ 0.00449421 │\n",
       "│ 3   │ 0.0604673 │ -2.85603e6 │ 35203.7    │\n",
       "│ 4   │ 5.37031   │ -2.85471e6 │ 1.55889e-5 │"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DataFrame(Runtime = runtime, Objective = objvals, Gradnorm = gradnrm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9  Q9. (10 pts) Compare with existing art\n",
    "Let's compare our method with lme4 package in R and MixedModels.jl package in Julia. Both lme4 and MixedModels.jl are developed mainly by Doug Bates. Summarize what you find."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "method  = [\"257\", \"lme4\", \"MixedModels.jl\"]\n",
    "runtime = zeros(3)  # record the run times\n",
    "loglike = zeros(3); # record the log-likelihood at MLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1  Your approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2.854709785203477e6"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm_257 = @benchmark fit!($lmm, $(NLopt.NLoptSolver(algorithm=:LD_MMA, \n",
    "        ftol_rel = 1e-12, ftol_abs = 1e-12, \n",
    "        xtol_rel = 1e-12, xtol_abs = 1e-12, \n",
    "        maxeval=10000))) setup=(init_ls!(lmm))\n",
    "runtime[1] = (median(bm_257).time) / 1e9\n",
    "loglike[1] = logl!(lmm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2  lme4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: RCall.jl: Loading required package: Matrix\n",
      "└ @ RCall /Users/caesar/.julia/packages/RCall/Qzssx/src/io.jl:160\n",
      "┌ Warning: RCall.jl: Parsed with column specification:\n",
      "│ cols(\n",
      "│   ID = col_double(),\n",
      "│   Y = col_double(),\n",
      "│   X1 = col_double(),\n",
      "│   X2 = col_double(),\n",
      "│   X3 = col_double(),\n",
      "│   X4 = col_double(),\n",
      "│   Z1 = col_double(),\n",
      "│   Z2 = col_double()\n",
      "│ )\n",
      "└ @ RCall /Users/caesar/.julia/packages/RCall/Qzssx/src/io.jl:160\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RObject{VecSxp}\n",
       "# A tibble: 1,753,910 x 8\n",
       "      ID      Y      X1      X2     X3      X4      Z1     Z2\n",
       "   <dbl>  <dbl>   <dbl>   <dbl>  <dbl>   <dbl>   <dbl>  <dbl>\n",
       " 1     1   2.30  2.16    1.71    1.95  -0.744   0.441  -0.155\n",
       " 2     1  -7.82  0.591   0.0736 -1.39  -1.53   -0.284  -0.283\n",
       " 3     1  -1.05 -0.666   0.223  -1.07   0.726  -0.701  -0.639\n",
       " 4     1  -5.42 -0.364   0.181   1.61  -0.623   0.0332  0.905\n",
       " 5     1  -4.23  0.880   1.91   -0.999 -0.371   1.03    0.581\n",
       " 6     1  -2.72  0.341   0.162   0.603 -0.285   1.42   -0.831\n",
       " 7     1   1.13 -0.0213 -0.500   0.178  0.747   0.638  -0.617\n",
       " 8     1 -17.6  -1.76    0.899  -0.931 -0.0527  0.353  -1.38 \n",
       " 9     1  -4.30 -0.207   0.248  -0.362 -0.0995 -0.587   0.266\n",
       "10     1  20.9   2.24    0.523  -1.47   2.07    0.714   2.06 \n",
       "# … with 1,753,900 more rows\n"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R\"\"\"\n",
    "library(lme4)\n",
    "library(readr)\n",
    "library(magrittr)\n",
    "\n",
    "testdata <- read_csv(\"lmm_data.txt\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RObject{RealSxp}\n",
       "   user  system elapsed \n",
       "103.371   8.921 112.949 \n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R\"\"\"\n",
    "rtime <- system.time(mmod <- \n",
    "  lmer(Y ~ X1 + X2 + X3 + X4 + (1 + Z1 + Z2 | ID), testdata, REML = FALSE))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "R\"\"\"\n",
    "rtime <- rtime[\"elapsed\"]\n",
    "summary(mmod)\n",
    "rlogl <- logLik(mmod)\n",
    "\"\"\"\n",
    "runtime[2] = @rget rtime\n",
    "loglike[2] = @rget rlogl;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3  MixedModels.jl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>ID</th><th>Y</th><th>X1</th><th>X2</th><th>X3</th><th>X4</th><th>Z1</th></tr><tr><th></th><th>String</th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th></tr></thead><tbody><p>1,753,910 rows × 8 columns (omitted printing of 1 columns)</p><tr><th>1</th><td>1</td><td>2.29562</td><td>2.16439</td><td>1.71386</td><td>1.94501</td><td>-0.743783</td><td>0.441018</td></tr><tr><th>2</th><td>1</td><td>-7.82426</td><td>0.591156</td><td>0.0735725</td><td>-1.38799</td><td>-1.53388</td><td>-0.28432</td></tr><tr><th>3</th><td>1</td><td>-1.05162</td><td>-0.665533</td><td>0.222603</td><td>-1.06705</td><td>0.726237</td><td>-0.700676</td></tr><tr><th>4</th><td>1</td><td>-5.41677</td><td>-0.363982</td><td>0.181063</td><td>1.6054</td><td>-0.623457</td><td>0.0332452</td></tr><tr><th>5</th><td>1</td><td>-4.22634</td><td>0.880121</td><td>1.91223</td><td>-0.998628</td><td>-0.370723</td><td>1.02984</td></tr><tr><th>6</th><td>1</td><td>-2.72221</td><td>0.341423</td><td>0.161899</td><td>0.602739</td><td>-0.285159</td><td>1.4209</td></tr><tr><th>7</th><td>1</td><td>1.13149</td><td>-0.0212729</td><td>-0.500254</td><td>0.177658</td><td>0.747107</td><td>0.638381</td></tr><tr><th>8</th><td>1</td><td>-17.6405</td><td>-1.76199</td><td>0.898998</td><td>-0.931399</td><td>-0.0526501</td><td>0.352804</td></tr><tr><th>9</th><td>1</td><td>-4.30473</td><td>-0.206732</td><td>0.248303</td><td>-0.362355</td><td>-0.0994689</td><td>-0.58678</td></tr><tr><th>10</th><td>1</td><td>20.9383</td><td>2.23638</td><td>0.523116</td><td>-1.46907</td><td>2.07343</td><td>0.714249</td></tr><tr><th>11</th><td>1</td><td>-4.59161</td><td>-1.02311</td><td>1.66654</td><td>0.766974</td><td>1.85919</td><td>2.02149</td></tr><tr><th>12</th><td>1</td><td>0.836847</td><td>0.676822</td><td>-0.0889856</td><td>-1.49619</td><td>-0.539135</td><td>0.0793583</td></tr><tr><th>13</th><td>1</td><td>-5.84599</td><td>-0.397877</td><td>-0.450184</td><td>0.349735</td><td>-0.542674</td><td>-0.565936</td></tr><tr><th>14</th><td>1</td><td>-6.23177</td><td>-2.0327</td><td>-0.608492</td><td>-0.0404538</td><td>0.88565</td><td>-1.67842</td></tr><tr><th>15</th><td>1</td><td>-5.9369</td><td>-1.38549</td><td>-1.80727</td><td>-1.31258</td><td>-0.0271809</td><td>2.2038</td></tr><tr><th>16</th><td>1</td><td>-5.32273</td><td>-0.61431</td><td>-0.0859859</td><td>0.970478</td><td>-0.0508639</td><td>2.38902</td></tr><tr><th>17</th><td>1</td><td>0.0758345</td><td>-0.122086</td><td>-2.02785</td><td>-0.161972</td><td>-0.805525</td><td>0.3504</td></tr><tr><th>18</th><td>1</td><td>7.52945</td><td>-0.298849</td><td>-2.7182</td><td>-1.06486</td><td>0.755002</td><td>-0.664704</td></tr><tr><th>19</th><td>1</td><td>-0.580833</td><td>0.0263333</td><td>-0.432225</td><td>-0.987936</td><td>0.0336734</td><td>-0.14892</td></tr><tr><th>20</th><td>1</td><td>-13.5485</td><td>-0.00237514</td><td>1.04502</td><td>-0.950582</td><td>-1.25265</td><td>-1.27643</td></tr><tr><th>21</th><td>1</td><td>-3.42085</td><td>-0.671995</td><td>-0.272527</td><td>-0.22161</td><td>0.18748</td><td>1.99234</td></tr><tr><th>22</th><td>1</td><td>-0.398223</td><td>0.939217</td><td>0.158931</td><td>0.211848</td><td>-0.627386</td><td>0.684831</td></tr><tr><th>23</th><td>1</td><td>-3.78251</td><td>-0.145142</td><td>-0.860611</td><td>0.143043</td><td>-0.534395</td><td>1.50549</td></tr><tr><th>24</th><td>1</td><td>5.18177</td><td>1.26779</td><td>0.440199</td><td>-0.342523</td><td>0.264829</td><td>0.979096</td></tr><tr><th>25</th><td>1</td><td>-10.2052</td><td>-0.839727</td><td>-0.00752814</td><td>1.03254</td><td>-1.02572</td><td>1.27587</td></tr><tr><th>26</th><td>1</td><td>3.2676</td><td>0.303675</td><td>0.398647</td><td>-2.24835</td><td>0.977578</td><td>-0.520845</td></tr><tr><th>27</th><td>1</td><td>6.64642</td><td>-0.259489</td><td>-0.822341</td><td>-0.488534</td><td>1.42375</td><td>-0.445779</td></tr><tr><th>28</th><td>1</td><td>6.98399</td><td>0.789707</td><td>-1.79072</td><td>-0.423215</td><td>-0.566674</td><td>0.176244</td></tr><tr><th>29</th><td>1</td><td>-16.3496</td><td>-0.735116</td><td>1.75313</td><td>-1.24459</td><td>-0.984253</td><td>-0.930345</td></tr><tr><th>30</th><td>1</td><td>-4.59682</td><td>-0.25318</td><td>0.0998096</td><td>0.682749</td><td>-0.141295</td><td>-1.47255</td></tr><tr><th>&vellip;</th><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td></tr></tbody></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|cccccccc}\n",
       "\t& ID & Y & X1 & X2 & X3 & X4 & Z1 & \\\\\n",
       "\t\\hline\n",
       "\t& String & Float64 & Float64 & Float64 & Float64 & Float64 & Float64 & \\\\\n",
       "\t\\hline\n",
       "\t1 & 1 & 2.29562 & 2.16439 & 1.71386 & 1.94501 & -0.743783 & 0.441018 & $\\dots$ \\\\\n",
       "\t2 & 1 & -7.82426 & 0.591156 & 0.0735725 & -1.38799 & -1.53388 & -0.28432 & $\\dots$ \\\\\n",
       "\t3 & 1 & -1.05162 & -0.665533 & 0.222603 & -1.06705 & 0.726237 & -0.700676 & $\\dots$ \\\\\n",
       "\t4 & 1 & -5.41677 & -0.363982 & 0.181063 & 1.6054 & -0.623457 & 0.0332452 & $\\dots$ \\\\\n",
       "\t5 & 1 & -4.22634 & 0.880121 & 1.91223 & -0.998628 & -0.370723 & 1.02984 & $\\dots$ \\\\\n",
       "\t6 & 1 & -2.72221 & 0.341423 & 0.161899 & 0.602739 & -0.285159 & 1.4209 & $\\dots$ \\\\\n",
       "\t7 & 1 & 1.13149 & -0.0212729 & -0.500254 & 0.177658 & 0.747107 & 0.638381 & $\\dots$ \\\\\n",
       "\t8 & 1 & -17.6405 & -1.76199 & 0.898998 & -0.931399 & -0.0526501 & 0.352804 & $\\dots$ \\\\\n",
       "\t9 & 1 & -4.30473 & -0.206732 & 0.248303 & -0.362355 & -0.0994689 & -0.58678 & $\\dots$ \\\\\n",
       "\t10 & 1 & 20.9383 & 2.23638 & 0.523116 & -1.46907 & 2.07343 & 0.714249 & $\\dots$ \\\\\n",
       "\t11 & 1 & -4.59161 & -1.02311 & 1.66654 & 0.766974 & 1.85919 & 2.02149 & $\\dots$ \\\\\n",
       "\t12 & 1 & 0.836847 & 0.676822 & -0.0889856 & -1.49619 & -0.539135 & 0.0793583 & $\\dots$ \\\\\n",
       "\t13 & 1 & -5.84599 & -0.397877 & -0.450184 & 0.349735 & -0.542674 & -0.565936 & $\\dots$ \\\\\n",
       "\t14 & 1 & -6.23177 & -2.0327 & -0.608492 & -0.0404538 & 0.88565 & -1.67842 & $\\dots$ \\\\\n",
       "\t15 & 1 & -5.9369 & -1.38549 & -1.80727 & -1.31258 & -0.0271809 & 2.2038 & $\\dots$ \\\\\n",
       "\t16 & 1 & -5.32273 & -0.61431 & -0.0859859 & 0.970478 & -0.0508639 & 2.38902 & $\\dots$ \\\\\n",
       "\t17 & 1 & 0.0758345 & -0.122086 & -2.02785 & -0.161972 & -0.805525 & 0.3504 & $\\dots$ \\\\\n",
       "\t18 & 1 & 7.52945 & -0.298849 & -2.7182 & -1.06486 & 0.755002 & -0.664704 & $\\dots$ \\\\\n",
       "\t19 & 1 & -0.580833 & 0.0263333 & -0.432225 & -0.987936 & 0.0336734 & -0.14892 & $\\dots$ \\\\\n",
       "\t20 & 1 & -13.5485 & -0.00237514 & 1.04502 & -0.950582 & -1.25265 & -1.27643 & $\\dots$ \\\\\n",
       "\t21 & 1 & -3.42085 & -0.671995 & -0.272527 & -0.22161 & 0.18748 & 1.99234 & $\\dots$ \\\\\n",
       "\t22 & 1 & -0.398223 & 0.939217 & 0.158931 & 0.211848 & -0.627386 & 0.684831 & $\\dots$ \\\\\n",
       "\t23 & 1 & -3.78251 & -0.145142 & -0.860611 & 0.143043 & -0.534395 & 1.50549 & $\\dots$ \\\\\n",
       "\t24 & 1 & 5.18177 & 1.26779 & 0.440199 & -0.342523 & 0.264829 & 0.979096 & $\\dots$ \\\\\n",
       "\t25 & 1 & -10.2052 & -0.839727 & -0.00752814 & 1.03254 & -1.02572 & 1.27587 & $\\dots$ \\\\\n",
       "\t26 & 1 & 3.2676 & 0.303675 & 0.398647 & -2.24835 & 0.977578 & -0.520845 & $\\dots$ \\\\\n",
       "\t27 & 1 & 6.64642 & -0.259489 & -0.822341 & -0.488534 & 1.42375 & -0.445779 & $\\dots$ \\\\\n",
       "\t28 & 1 & 6.98399 & 0.789707 & -1.79072 & -0.423215 & -0.566674 & 0.176244 & $\\dots$ \\\\\n",
       "\t29 & 1 & -16.3496 & -0.735116 & 1.75313 & -1.24459 & -0.984253 & -0.930345 & $\\dots$ \\\\\n",
       "\t30 & 1 & -4.59682 & -0.25318 & 0.0998096 & 0.682749 & -0.141295 & -1.47255 & $\\dots$ \\\\\n",
       "\t$\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ &  \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "1753910×8 DataFrame. Omitted printing of 3 columns\n",
       "│ Row     │ ID     │ Y        │ X1         │ X2         │ X3        │\n",
       "│         │ \u001b[90mString\u001b[39m │ \u001b[90mFloat64\u001b[39m  │ \u001b[90mFloat64\u001b[39m    │ \u001b[90mFloat64\u001b[39m    │ \u001b[90mFloat64\u001b[39m   │\n",
       "├─────────┼────────┼──────────┼────────────┼────────────┼───────────┤\n",
       "│ 1       │ 1      │ 2.29562  │ 2.16439    │ 1.71386    │ 1.94501   │\n",
       "│ 2       │ 1      │ -7.82426 │ 0.591156   │ 0.0735725  │ -1.38799  │\n",
       "│ 3       │ 1      │ -1.05162 │ -0.665533  │ 0.222603   │ -1.06705  │\n",
       "│ 4       │ 1      │ -5.41677 │ -0.363982  │ 0.181063   │ 1.6054    │\n",
       "│ 5       │ 1      │ -4.22634 │ 0.880121   │ 1.91223    │ -0.998628 │\n",
       "│ 6       │ 1      │ -2.72221 │ 0.341423   │ 0.161899   │ 0.602739  │\n",
       "│ 7       │ 1      │ 1.13149  │ -0.0212729 │ -0.500254  │ 0.177658  │\n",
       "│ 8       │ 1      │ -17.6405 │ -1.76199   │ 0.898998   │ -0.931399 │\n",
       "│ 9       │ 1      │ -4.30473 │ -0.206732  │ 0.248303   │ -0.362355 │\n",
       "│ 10      │ 1      │ 20.9383  │ 2.23638    │ 0.523116   │ -1.46907  │\n",
       "⋮\n",
       "│ 1753900 │ 1000   │ 2.34091  │ 1.49228    │ 0.682125   │ 0.302774  │\n",
       "│ 1753901 │ 1000   │ -3.18819 │ -0.689118  │ -0.612859  │ 0.0738376 │\n",
       "│ 1753902 │ 1000   │ -11.3465 │ 0.763788   │ 2.90449    │ 0.162727  │\n",
       "│ 1753903 │ 1000   │ 8.6989   │ 1.51636    │ 0.137675   │ 0.308407  │\n",
       "│ 1753904 │ 1000   │ -18.9671 │ 0.145233   │ 1.34346    │ -2.42149  │\n",
       "│ 1753905 │ 1000   │ -0.1842  │ -0.916041  │ -0.586905  │ 0.77127   │\n",
       "│ 1753906 │ 1000   │ -6.04537 │ -1.28779   │ 0.165535   │ 0.24736   │\n",
       "│ 1753907 │ 1000   │ 2.55843  │ -0.19814   │ -0.0349219 │ 0.48918   │\n",
       "│ 1753908 │ 1000   │ 3.49073  │ -0.0606105 │ 0.199115   │ 1.30314   │\n",
       "│ 1753909 │ 1000   │ 1.58389  │ -0.716483  │ -0.864964  │ 0.0262824 │\n",
       "│ 1753910 │ 1000   │ 2.16344  │ -0.889437  │ -0.966382  │ -0.842096 │"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testdata = CSV.File(\"lmm_data.txt\", types = Dict(1=>String)) |> DataFrame!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2465281625"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mj = fit(MixedModel, @formula(Y ~ X1 + X2 + X3 + X4 + (1 + Z1 + Z2 | ID)), testdata)\n",
    "bm_mm = @benchmark fit(MixedModel, @formula(Y ~ X1 + X2 + X3 + X4 + (1 + Z1 + Z2 | ID)), testdata)\n",
    "loglike[3] = loglikelihood(mj)\n",
    "runtime[3] = median(bm_mm).time / 1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: \n",
       "  memory estimate:  920.42 MiB\n",
       "  allocs estimate:  9433918\n",
       "  --------------\n",
       "  minimum time:     1.181 s (8.34% GC)\n",
       "  median time:      1.247 s (15.48% GC)\n",
       "  mean time:        1.272 s (15.19% GC)\n",
       "  maximum time:     1.415 s (20.39% GC)\n",
       "  --------------\n",
       "  samples:          4\n",
       "  evals/sample:     1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Linear mixed model fit by maximum likelihood\n",
       " Y ~ 1 + X1 + X2 + X3 + X4 + (1 + Z1 + Z2 | ID)\n",
       "     logLik        -2 logLik          AIC             BIC       \n",
       " -2.85470979×10⁶  5.70941957×10⁶  5.70944357×10⁶   5.7095921×10⁶\n",
       "\n",
       "Variance components:\n",
       "            Column    Variance  Std.Dev.   Corr.\n",
       "ID       (Intercept)  2.0568343 1.4341668\n",
       "         Z1           1.1823375 1.0873534 -0.01\n",
       "         Z2           0.9379257 0.9684656  0.01 -0.02\n",
       "Residual              1.4990426 1.2243540\n",
       " Number of obs: 1753910; levels of grouping factors: 1000\n",
       "\n",
       "  Fixed-effects parameters:\n",
       "──────────────────────────────────────────────────────────\n",
       "               Estimate    Std.Error      z value  P(>|z|)\n",
       "──────────────────────────────────────────────────────────\n",
       "(Intercept)   0.0823213  0.045357         1.81496   0.0695\n",
       "X1            6.50014    0.000924999   7027.19      <1e-99\n",
       "X2           -3.49879    0.000924895  -3782.91      <1e-99\n",
       "X3            1.0005     0.000925797   1080.69      <1e-99\n",
       "X4            5.00005    0.000925235   5404.09      <1e-99\n",
       "──────────────────────────────────────────────────────────"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(bm_mm)\n",
    "mj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.4  Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>method</th><th>runtime</th><th>logl</th></tr><tr><th></th><th>String</th><th>Float64</th><th>Float64</th></tr></thead><tbody><p>3 rows × 3 columns</p><tr><th>1</th><td>257</td><td>0.156089</td><td>-2.85471e6</td></tr><tr><th>2</th><td>lme4</td><td>112.949</td><td>-2.85471e6</td></tr><tr><th>3</th><td>MixedModels.jl</td><td>1.24653</td><td>-2.85471e6</td></tr></tbody></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ccc}\n",
       "\t& method & runtime & logl\\\\\n",
       "\t\\hline\n",
       "\t& String & Float64 & Float64\\\\\n",
       "\t\\hline\n",
       "\t1 & 257 & 0.156089 & -2.85471e6 \\\\\n",
       "\t2 & lme4 & 112.949 & -2.85471e6 \\\\\n",
       "\t3 & MixedModels.jl & 1.24653 & -2.85471e6 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "3×3 DataFrame\n",
       "│ Row │ method         │ runtime  │ logl       │\n",
       "│     │ \u001b[90mString\u001b[39m         │ \u001b[90mFloat64\u001b[39m  │ \u001b[90mFloat64\u001b[39m    │\n",
       "├─────┼────────────────┼──────────┼────────────┤\n",
       "│ 1   │ 257            │ 0.156089 │ -2.85471e6 │\n",
       "│ 2   │ lme4           │ 112.949  │ -2.85471e6 │\n",
       "│ 3   │ MixedModels.jl │ 1.24653  │ -2.85471e6 │"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DataFrame(method = method, runtime = runtime, logl = loglike)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My Summary\n",
    "\n",
    "First of all, the Julia implementation is almost 100 folds faster than the R linear mixed model package lme4, as shown above in the table (given my machine's environment and conditions). From personal experience I can confirm lme4, as fast as it is compared to other R packages like nlme, it is still quite slow. Julia's MixedModels.jl finishing at around 1.25 seconds is impressive. However, due to the fact that we can further optimize it in this special case, our code gave us another almost 10 fold improvement. Specialization would of course bring in further optimization, at the expense of generality. Thus, it is nice that Julia's linear mixed model package, without the loss of generality, finds a sweet spot and optimize computations to almost as fast as C. Even though not general enough to replace MixedModels.jl, our implementation taught us how to optimize our code for any specific research project. It shows us that using a faster solfware alone is not enough to achieve the perfect optimization. Optimized softwares like Julia provides us with a platform and tools to implement our ideas efficiently. It is up to the user to use these tools to their fullest extent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10  Q10. Be proud of yourself\n",
    "Go to your resume/cv and claim you have experience performing analysis on complex longitudinal data sets with millions of records. And you beat current software by XXX fold.     \n",
    "\n",
    "Answer: I definitely am. That was a biiig hw, but I'm glad I finally did it. Really with 257 was broken up into 2 or 3 classes so we can learn everything in depth instead of cramming through."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.4.2",
   "language": "julia",
   "name": "julia-1.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
